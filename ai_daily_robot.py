import requests
from bs4 import BeautifulSoup
import os
from openai import OpenAI
import json
from datetime import datetime
import re
import time
import sys
import logging
import argparse
import signal
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import feedparser

# å°è¯•å¯¼å…¥APSchedulerï¼Œå¦‚æœå¤±è´¥åˆ™æç¤ºç”¨æˆ·å®‰è£…
try:
    from apscheduler.schedulers.background import BackgroundScheduler
    from apscheduler.triggers.cron import CronTrigger
    APSCHEDULER_AVAILABLE = True
except ImportError:
    APSCHEDULER_AVAILABLE = False
    print("[WARNING] APScheduleræœªå®‰è£…ï¼Œå®šæ—¶ä»»åŠ¡åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install APScheduler>=3.10.0")

# é…ç½®å‚æ•°
NEWS_SOURCES = [
    # ä¸­æ–‡æ–°é—»æº (ä¼˜å…ˆçº§: 1-5, 5æœ€é«˜)
    {"url": "https://36kr.com", "name": "36æ°ª", "enabled": True, "priority": 5},
    {"url": "https://www.jiqizhixin.com/", "name": "æœºå™¨ä¹‹å¿ƒ", "enabled": True, "priority": 5},
    {"url": "https://www.aminer.cn/topic/ai", "name": "AMiner", "enabled": True, "priority": 4},
    {"url": "https://www.infoq.cn/topic/AI&LLM", "name": "InfoQ", "enabled": True, "priority": 3},
    {"url": "https://www.leiphone.com/category/ai", "name": "é›·é”‹ç½‘", "enabled": True, "priority": 4},

    # è‹±æ–‡æ–°é—»æº
    {"url": "https://venturebeat.com/category/ai/", "name": "VentureBeat", "enabled": True, "priority": 4},
    {"url": "https://techcrunch.com/category/artificial-intelligence/", "name": "TechCrunch", "enabled": True, "priority": 4},

    # RSSæº (é€šå¸¸æ›´æ–°é¢‘ç¹ï¼Œä¼˜å…ˆçº§ç¨ä½)
    {"url": "https://feeds.feedburner.com/venturebeat/SZYF", "name": "VentureBeat RSS", "enabled": True, "priority": 3},
    {"url": "https://techcrunch.com/feed/", "name": "TechCrunch RSS", "enabled": True, "priority": 3},
    {"url": "https://www.artificialintelligence-news.com/feed/", "name": "AI News RSS", "enabled": True, "priority": 2},
    {"url": "https://www.mit.edu/~jintao/ai_news.xml", "name": "MIT AI News RSS", "enabled": True, "priority": 2},

    # APIæº (éœ€è¦APIå¯†é’¥)
    # {"url": "https://newsapi.org/v2/everything?q=artificial+intelligence&language=en&sortBy=publishedAt&apiKey=YOUR_API_KEY", "name": "NewsAPI", "enabled": False, "priority": 3},
    # {"url": "https://gnews.io/api/v4/search?q=artificial+intelligence&token=YOUR_API_KEY", "name": "GNews API", "enabled": False, "priority": 3},
    # {"url": "https://api.currentsapi.services/v1/search?keywords=artificial+intelligence&apiKey=YOUR_API_KEY", "name": "CurrentsAPI", "enabled": False, "priority": 2},
]

MAX_ARTICLES_PER_SOURCE = 10  # æ¯ä¸ªæºæœ€å¤šè·å–çš„æ–‡ç« æ•°é‡
MAX_TOTAL_ARTICLES = 30       # æ€»å…±æœ€å¤šè¿”å›çš„æ–‡ç« æ•°é‡
MAX_ARTICLES_PER_PRIORITY = {  # æ¯ä¸ªä¼˜å…ˆçº§æœ€å¤šè¿”å›çš„æ–‡ç« æ•°é‡
    5: 8,   # é«˜ä¼˜å…ˆçº§æºæœ€å¤š8æ¡
    4: 6,   # ä¸­é«˜ä¼˜å…ˆçº§æºæœ€å¤š6æ¡
    3: 4,   # ä¸­ä¼˜å…ˆçº§æºæœ€å¤š4æ¡
    2: 2,   # ä½ä¼˜å…ˆçº§æºæœ€å¤š2æ¡
    1: 1    # æœ€ä½ä¼˜å…ˆçº§æºæœ€å¤š1æ¡
}
AI_KEYWORDS = [
    "ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
    "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
    "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹", "ç¥ç»ç½‘ç»œ"
]

REQUEST_TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
RETRY_COUNT = 3      # é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2      # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰

# å®šæ—¶ä»»åŠ¡é…ç½®
SCHEDULER_CONFIG = {
    "enabled": False,  # æ˜¯å¦å¯ç”¨å®šæ—¶ä»»åŠ¡
    "timezone": "Asia/Shanghai",  # æ—¶åŒºè®¾ç½®
    "cron_expression": "0 9 * * *",  # é»˜è®¤æ¯å¤©ä¸Šåˆ9ç‚¹æ‰§è¡Œ
    "max_instances": 1,  # æœ€å¤§å¹¶å‘å®ä¾‹æ•°
    "misfire_grace_time": 3600,  # é”™è¿‡æ‰§è¡Œæ—¶é—´çš„å®½å®¹æ—¶é—´ï¼ˆç§’ï¼‰
}

# æ—¥å¿—é…ç½®
LOG_CONFIG = {
    "level": "INFO",  # æ—¥å¿—çº§åˆ«: DEBUG, INFO, WARNING, ERROR
    "file": "ai_daily_robot.log",  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    "max_size": 10485760,  # æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆ10MBï¼‰
    "backup_count": 5,  # ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°é‡
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
}

# å¤šWebhooké…ç½®
WEBHOOK_CONFIGS = [
    {
        "name": "ä¸»ç¾¤èŠ",
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/4d6b0afa-c34f-4b0f-9bb3-1716bc2ed4a3",
        "enabled": True,
        "send_image": True
    },
    {
        "name": "æµ‹è¯•ç¾¤èŠ", 
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/02880094-6e28-4dea-a815-ff02fea49072",
        "enabled": True,
        "send_image": True
    },
    {
        "name": "å¤‡ä»½ç¾¤èŠ",
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/your_backup_webhook_url_here", 
        "enabled": False,
        "send_image": False
    }
]

# é…ç½®OpenAI API
# è¯·å°†ä¸‹é¢çš„"your_actual_api_key_here"æ›¿æ¢ä¸ºæ‚¨ä»deepseekè·å–çš„å®é™…APIå¯†é’¥
API_KEY = "sk-e9c92f4884e742c1a533d17c1ab729d0"
client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com/")

# å…¨å±€å˜é‡
scheduler = None
logger = None
shutdown_event = None

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    try:
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(LOG_CONFIG["file"])
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, LOG_CONFIG["level"]),
            format=LOG_CONFIG["format"],
            handlers=[
                logging.FileHandler(LOG_CONFIG["file"], encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        global logger
        logger = logging.getLogger(__name__)
        logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return logger
    except Exception as e:
        print(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„logger
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…å…³é—­"""
    global logger, scheduler, shutdown_event
    
    if logger:
        logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡å…³é—­ç¨‹åº...")
    else:
        print(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡å…³é—­ç¨‹åº...")
    
    if shutdown_event:
        shutdown_event.set()
    
    if scheduler:
        scheduler.shutdown(wait=True)
    
    if logger:
        logger.info("ç¨‹åºå·²å®‰å…¨å…³é—­")
    else:
        print("ç¨‹åºå·²å®‰å…¨å…³é—­")
    
    sys.exit(0)

def setup_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    global scheduler
    
    if not APSCHEDULER_AVAILABLE:
        if logger:
            logger.error("APSchedulerä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨å®šæ—¶ä»»åŠ¡")
        else:
            print("APSchedulerä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨å®šæ—¶ä»»åŠ¡")
        return None
    
    try:
        scheduler = BackgroundScheduler(
            timezone=SCHEDULER_CONFIG["timezone"],
            max_instances=SCHEDULER_CONFIG["max_instances"]
        )
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        scheduler.add_job(
            func=execute_scheduled_task,
            trigger=CronTrigger.from_crontab(SCHEDULER_CONFIG["cron_expression"]),
            id='ai_daily_robot_task',
            name='AIæ—¥æŠ¥æœºå™¨äººå®šæ—¶ä»»åŠ¡',
            replace_existing=True,
            misfire_grace_time=SCHEDULER_CONFIG["misfire_grace_time"]
        )
        
        if logger:
            logger.info(f"å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²è®¾ç½®ï¼Œcronè¡¨è¾¾å¼: {SCHEDULER_CONFIG['cron_expression']}")
        else:
            print(f"å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²è®¾ç½®ï¼Œcronè¡¨è¾¾å¼: {SCHEDULER_CONFIG['cron_expression']}")
        
        return scheduler
    except Exception as e:
        if logger:
            logger.error(f"è®¾ç½®å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¤±è´¥: {e}")
        else:
            print(f"è®¾ç½®å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¤±è´¥: {e}")
        return None

def execute_scheduled_task():
    """æ‰§è¡Œå®šæ—¶ä»»åŠ¡çš„å‡½æ•°"""
    global logger
    
    try:
        if logger:
            logger.info("=" * 60)
            logger.info(f"[SCHEDULED_TASK] å¼€å§‹æ‰§è¡Œå®šæ—¶AIæ—¥æŠ¥ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 60)
        else:
            print("=" * 60)
            print(f"[SCHEDULED_TASK] å¼€å§‹æ‰§è¡Œå®šæ—¶AIæ—¥æŠ¥ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 60)
        
        # æ‰§è¡Œä¸»è¦çš„AIæ—¥æŠ¥ä»»åŠ¡
        execute_ai_robot_task()
        
        if logger:
            logger.info("=" * 60)
            logger.info(f"[SCHEDULED_TASK] å®šæ—¶AIæ—¥æŠ¥ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 60)
        else:
            print("=" * 60)
            print(f"[SCHEDULED_TASK] å®šæ—¶AIæ—¥æŠ¥ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 60)
            
    except Exception as e:
        if logger:
            logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        else:
            print(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

def execute_ai_robot_task():
    """æ‰§è¡ŒAIæœºå™¨äººæ ¸å¿ƒä»»åŠ¡"""
    global logger
    
    try:
        # é£ä¹¦åº”ç”¨å‡­è¯
        feishu_app_id = "cli_a8ef4e27bd85900b"
        feishu_app_secret = "By4Y7Z2NpQvovyJ0Efp2CgyOF8dAC7bV"
        
        if logger:
            logger.info(f"é£ä¹¦åº”ç”¨ID: {feishu_app_id}")
        else:
            print(f"é£ä¹¦åº”ç”¨ID: {feishu_app_id}")
        
        # è·å–tenant_access_token
        if logger:
            logger.info("æ­£åœ¨è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ...")
        else:
            print("æ­£åœ¨è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ...")
        
        access_token = get_tenant_access_token(feishu_app_id, feishu_app_secret)
        if not access_token:
            if logger:
                logger.error("æ— æ³•è·å–access_tokenï¼Œä»»åŠ¡å¤±è´¥ã€‚")
            else:
                print("æ— æ³•è·å–access_tokenï¼Œä»»åŠ¡å¤±è´¥ã€‚")
            return False
        
        if logger:
            logger.info("æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œ")
        else:
            print("æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œ")

        # è·å–AIæ–°é—»
        if logger:
            logger.info("å¼€å§‹è·å–AIæ–°é—»...")
        else:
            print("å¼€å§‹è·å–AIæ–°é—»...")
        
        ai_news = get_ai_news()
        
        if ai_news:
            if logger:
                logger.info(f"æˆåŠŸè·å– {len(ai_news)} æ¡æ–°é—»")
            else:
                print(f"æˆåŠŸè·å– {len(ai_news)} æ¡æ–°é—»")
            
            # ç”Ÿæˆæ‘˜è¦
            if logger:
                logger.info("æ­£åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦...")
            else:
                print("æ­£åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦...")
            
            summary = summarize_news(ai_news)
            
            if logger:
                logger.info("ç”Ÿæˆçš„æ—¥æŠ¥æ‘˜è¦:")
                logger.info("-" * 40)
                logger.info(summary)
                logger.info("-" * 40)
            else:
                print("ç”Ÿæˆçš„æ—¥æŠ¥æ‘˜è¦:")
                print("-" * 40)
                print(summary)
                print("-" * 40)
            
            # ä¸Šä¼ ä¸»é¢˜å›¾ç‰‡
            image_path = "/home/ubuntu/upload/search_images/QkPqdKuxZOlT.jpg"
            image_key = None
            if os.path.exists(image_path):
                if logger:
                    logger.info(f"æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸Šä¼ : {image_path}")
                else:
                    print(f"æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸Šä¼ : {image_path}")
                image_key = upload_image_to_feishu(image_path, access_token)
            else:
                if logger:
                    logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}ï¼Œéƒ¨åˆ†webhookå°†ä¸åŒ…å«å›¾ç‰‡")
                else:
                    print(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}ï¼Œéƒ¨åˆ†webhookå°†ä¸åŒ…å«å›¾ç‰‡")
            
            # æ˜¾ç¤ºå½“å‰webhooké…ç½®
            print_webhook_configs()
            
            # å‘é€åˆ°å¤šä¸ªwebhook
            if logger:
                logger.info("å¼€å§‹å‘é€æ¶ˆæ¯åˆ°å¤šä¸ªé£ä¹¦ç¾¤èŠ...")
            else:
                print("å¼€å§‹å‘é€æ¶ˆæ¯åˆ°å¤šä¸ªé£ä¹¦ç¾¤èŠ...")
            
            success_count, failed_count = send_to_multiple_webhooks(summary, ai_news, image_key)
            
            if logger:
                logger.info(f"Webhookå‘é€ç»“æœç»Ÿè®¡: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            else:
                print(f"Webhookå‘é€ç»“æœç»Ÿè®¡: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            
            return True
        else:
            if logger:
                logger.error("æœªèƒ½è·å–AIæ–°é—»")
            else:
                print("æœªèƒ½è·å–AIæ–°é—»")
            return False
            
    except Exception as e:
        if logger:
            logger.error(f"AIæœºå™¨äººä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        else:
            print(f"AIæœºå™¨äººä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False

def get_tenant_access_token(app_id, app_secret):
    """è·å–é£ä¹¦åº”ç”¨çš„tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            token = result["tenant_access_token"]
            print("æˆåŠŸè·å–tenant_access_token")
            return token
        else:
            print("è·å–tenant_access_tokenå¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"è·å–tenant_access_tokenæ—¶å‡ºé”™: {e}")
        return None

def get_ai_news_from_source(url, source_name="æœºå™¨ä¹‹å¿ƒ"):
    """ä»æŒ‡å®šURLè·å–AIæ–°é—»"""
    try:
        print(f"[DEBUG] å¼€å§‹ä» {source_name} è·å–æ–°é—»...")
        print(f"[DEBUG] è¯·æ±‚URL: {url}")
        
        # æ ¹æ®ä¸åŒçš„æ•°æ®æºä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
        if source_name == "æœºå™¨ä¹‹å¿ƒ":
            return get_jiqizhixin_news(url, source_name)
        elif source_name == "36æ°ª":
            return get_36kr_news(url, source_name)
        elif source_name == "InfoQ":
            return get_infoq_news(url, source_name)
        elif source_name == "AMiner":
            return get_aminer_news(url, source_name)
        elif source_name == "é›·é”‹ç½‘":
            return get_leiphone_news(url, source_name)
        elif source_name == "VentureBeat":
            return get_venturebeat_news(url, source_name)
        elif source_name == "TechCrunch":
            return get_techcrunch_news(url, source_name)
        elif source_name.endswith("RSS") or "rss" in source_name.lower():
            return get_rss_news(url, source_name)
        elif source_name.endswith("API") or "api" in source_name.lower():
            return get_api_news(url, source_name)
        else:
            return get_generic_news(url, source_name)
    except Exception as e:
        print(f"[ERROR] ä» {source_name} è·å–æ–°é—»æ—¶å‡ºé”™ {url}: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return []

def get_jiqizhixin_news(url, source_name):
    """è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»"""
    try:
        print(f"[DEBUG] {source_name}: å¼€å§‹ä½¿ç”¨Seleniumè·å–æ–°é—»...")
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        print(f"[DEBUG] {source_name}: æ­£åœ¨åˆå§‹åŒ–Chrome WebDriver...")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        print(f"[DEBUG] {source_name}: WebDriveråˆå§‹åŒ–æˆåŠŸï¼Œæ­£åœ¨è®¿é—®é¡µé¢...")
        driver.get(url)
        
        wait = WebDriverWait(driver, 10)
        print(f"[DEBUG] {source_name}: ç­‰å¾…é¡µé¢å…ƒç´ åŠ è½½...")
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "home__left-body")))
        print(f"[DEBUG] {source_name}: é¡µé¢å…ƒç´ åŠ è½½æˆåŠŸï¼Œè·å–é¡µé¢æºç ...")
        
        page_source = driver.page_source
        print(f"[DEBUG] {source_name}: é¡µé¢æºç é•¿åº¦: {len(page_source)} å­—ç¬¦")
        driver.quit()
        print(f"[DEBUG] {source_name}: WebDriverå·²å…³é—­")
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_="body-title")
        print(f"[DEBUG] {source_name}: æ‰¾åˆ° {len(news_items)} ä¸ªbody-titleç±»é“¾æ¥")
        
        if len(news_items) < 5:
            more_items = soup.find_all("a", class_=["article-item", "news-item", "post-title", "title"])
            print(f"[DEBUG] {source_name}: é¢å¤–æ‰¾åˆ° {len(more_items)} ä¸ªé€šç”¨æ–‡ç« é“¾æ¥")
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            potential_titles = soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"title|heading|headline"))
            more_items = soup.find_all("a", href=re.compile(r"articles|news|post|reference"))
            print(f"[DEBUG] {source_name}: å†æ‰¾åˆ° {len(more_items)} ä¸ªæ½œåœ¨æ–°é—»é“¾æ¥")
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            ai_links = []
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and ("ai" in text.lower() or "æœºå™¨" in text or "æ™ºèƒ½" in text or "AI" in text):
                    ai_links.append(link)
            print(f"[DEBUG] {source_name}: æ‰¾åˆ° {len(ai_links)} ä¸ªAIç›¸å…³é“¾æ¥")
            news_items.extend(ai_links)
        
        print(f"[DEBUG] {source_name}: æ€»å…±æ‰¾åˆ° {len(news_items)} ä¸ªå€™é€‰é“¾æ¥")
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.jiqizhixin.com" + link
                    else:
                        link = "https://www.jiqizhixin.com/" + link
            else:
                onclick = item.get("onclick", "")
                if onclick:
                    match = re.search(r"'(https?://[^']+)'", onclick)
                    if match:
                        link = match.group(1)
                    else:
                        match = re.search(r"'(/articles/[^']+)'", onclick)
                        if match:
                            link = "https://www.jiqizhixin.com" + match.group(1)
                if not link:
                    data_href = item.get("data-href", "")
                    if data_href:
                        if not data_href.startswith("http"):
                            if data_href.startswith("/"):
                                link = "https://www.jiqizhixin.com" + data_href
                            else:
                                link = "https://www.jiqizhixin.com/" + data_href
                        else:
                            link = data_href
            
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            parent = item.find_parent()
            if parent:
                date_elements = parent.find_all(string=re.compile(r"\d{1,2}æœˆ\d{1,2}æ—¥"))
                if date_elements:
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", str(date_elements[0]))
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        print(f"[DEBUG] {source_name}: æˆåŠŸæå– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"[ERROR] è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return []

def get_36kr_news(url, source_name):
    """è·å–36æ°ªAIæ–°é—»"""
    try:
        print(f"[DEBUG] {source_name}: å¼€å§‹ä½¿ç”¨requestsè·å–æ–°é—»...")
        # å…ˆå°è¯•ä½¿ç”¨requestsè·å–
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        print(f"[DEBUG] {source_name}: å‘é€HTTPè¯·æ±‚...")
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        print(f"[DEBUG] {source_name}: HTTPå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"[DEBUG] {source_name}: å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„ç±»åå’Œé€‰æ‹©å™¨
        news_items = []
        
        # æ–¹æ³•1ï¼šå¯»æ‰¾æ–‡ç« é“¾æ¥
        news_items.extend(soup.find_all("a", class_=["item-title", "article-title", "title", "post-title"]))
        print(f"[DEBUG] {source_name}: æ–¹æ³•1æ‰¾åˆ° {len(news_items)} ä¸ªæ ‡é¢˜é“¾æ¥")
        
        # æ–¹æ³•2ï¼šå¯»æ‰¾åŒ…å«AIå…³é”®è¯çš„é“¾æ¥
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and any(keyword in text.lower() for keyword in AI_KEYWORDS):
                    news_items.append(link)
            print(f"[DEBUG] {source_name}: æ–¹æ³•2æ‰¾åˆ° {len([x for x in news_items if soup.find_all('a', href=True)])} ä¸ªAIç›¸å…³é“¾æ¥")
        
        # æ–¹æ³•3ï¼šå¯»æ‰¾ç‰¹å®šçš„æ–‡ç« å®¹å™¨
        if len(news_items) < 5:
            news_items.extend(soup.find_all("div", class_=re.compile(r"item|article|post")))
            print(f"[DEBUG] {source_name}: æ–¹æ³•3æ‰¾åˆ° {len([x for x in soup.find_all('div', class_=re.compile(r'item|article|post'))])} ä¸ªæ–‡ç« å®¹å™¨")
        
        print(f"[DEBUG] {source_name}: æ€»å…±æ‰¾åˆ° {len(news_items)} ä¸ªå€™é€‰é¡¹ç›®")
        
        for item in news_items:
            # å¦‚æœæ˜¯divå®¹å™¨ï¼Œéœ€è¦ä»ä¸­æå–é“¾æ¥
            if item.name == "div":
                link_element = item.find("a")
                if not link_element:
                    continue
                title = link_element.get_text(strip=True)
                link = link_element.get("href", "")
            else:
                title = item.get_text(strip=True)
                link = item.get("href", "")
            
            if len(title) < 5:
                continue
                
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://36kr.com" + link
                    else:
                        link = "https://36kr.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ—¥æœŸ
            parent = item.find_parent()
            if parent:
                time_element = parent.find("time") or parent.find("span", class_=re.compile(r"time|date"))
                if time_element:
                    date_text = time_element.get_text(strip=True)
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", date_text)
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        print(f"[DEBUG] {source_name}: æˆåŠŸæå– {len(articles)} ç¯‡AIç›¸å…³æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"[ERROR] è·å–36æ°ªæ–°é—»æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return []

def get_infoq_news(url, source_name):
    """è·å–InfoQ AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["news-title", "article-title", "title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.infoq.cn" + link
                    else:
                        link = "https://www.infoq.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–InfoQæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_aminer_news(url, source_name):
    """è·å–AMiner AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "paper-title", "article-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.aminer.cn" + link
                    else:
                        link = "https://www.aminer.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–AMineræ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_leiphone_news(url, source_name):
    """è·å–é›·é”‹ç½‘AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.leiphone.com" + link
                    else:
                        link = "https://www.leiphone.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–é›·é”‹ç½‘æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_venturebeat_news(url, source_name):
    """è·å–VentureBeat AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://venturebeat.com" + link
                else:
                    link = "https://venturebeat.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–VentureBeatæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_techcrunch_news(url, source_name):
    """è·å–TechCrunch AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://techcrunch.com" + link
                else:
                    link = "https://techcrunch.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–TechCrunchæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_generic_news(url, source_name):
    """é€šç”¨æ–°é—»è·å–æ–¹æ³•"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        
        wait = WebDriverWait(driver, 10)
        
        page_source = driver.page_source
        driver.quit()
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–é€šç”¨æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_rss_news(url, source_name):
    """è·å–RSS/ATOMæ–°é—»"""
    try:
        print(f"[DEBUG] {source_name}: å¼€å§‹è§£æRSSæº...")
        feed = feedparser.parse(url)
        print(f"[DEBUG] {source_name}: RSSæºåŒ…å« {len(feed.entries)} ä¸ªæ¡ç›®")
        articles = []
        
        for entry in feed.entries:
            title = entry.get('title', '')
            if not title or len(title.strip()) < 5:
                continue
                
            link = entry.get('link', '')
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            title_lower = title.lower()
            ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                          "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                          "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
            
            if not any(keyword in title_lower for keyword in ai_keywords):
                continue
            
            # è·å–æ—¥æœŸ
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            if 'published' in entry:
                try:
                    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z')
                    date = pub_date.strftime("%mæœˆ%dæ—¥")
                except:
                    pass
            
            articles.append({
                "title": title.strip(),
                "link": link,
                "date": date,
                "source": source_name
            })
        
        print(f"[DEBUG] {source_name}: æˆåŠŸæå– {len(articles)} ç¯‡AIç›¸å…³æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"[ERROR] è·å–RSSæ–°é—»æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return []

def get_api_news(url, source_name):
    """è·å–APIæ–°é—»"""
    try:
        print(f"[DEBUG] {source_name}: å¼€å§‹è¯·æ±‚API...")
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers)
        print(f"[DEBUG] {source_name}: APIå“åº”çŠ¶æ€ç : {response.status_code}")
        response.raise_for_status()
        
        data = response.json()
        print(f"[DEBUG] {source_name}: APIå“åº”æ•°æ®ç±»å‹: {type(data)}")
        articles = []
        
        # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
        if isinstance(data, dict):
            # å°è¯•ä¸åŒçš„APIå“åº”æ ¼å¼
            articles_data = []
            
            # æ ¼å¼1: {articles: [...]}
            if 'articles' in data:
                articles_data = data['articles']
            # æ ¼å¼2: {data: [...]}
            elif 'data' in data and isinstance(data['data'], list):
                articles_data = data['data']
            # æ ¼å¼3: {items: [...]}
            elif 'items' in data:
                articles_data = data['items']
            # æ ¼å¼4: {results: [...]}
            elif 'results' in data:
                articles_data = data['results']
            # æ ¼å¼5: {response: {docs: [...]}}
            elif 'response' in data and isinstance(data['response'], dict) and 'docs' in data['response']:
                articles_data = data['response']['docs']
            
            for item in articles_data:
                if isinstance(item, dict):
                    title = item.get('title', item.get('headline', item.get('name', '')))
                    link = item.get('url', item.get('link', item.get('href', '')))
                    
                    if not title or len(title.strip()) < 5:
                        continue
                    
                    if not link:
                        continue
                    
                    # ç­›é€‰AIç›¸å…³æ–°é—»
                    title_lower = title.lower()
                    ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                                  "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                                  "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
                    
                    if not any(keyword in title_lower for keyword in ai_keywords):
                        continue
                    
                    # è·å–æ—¥æœŸ
                    date = datetime.now().strftime("%mæœˆ%dæ—¥")
                    pub_date = item.get('publishedAt', item.get('published_date', item.get('date', '')))
                    if pub_date:
                        try:
                            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
                            for date_format in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d %H:%M:%S']:
                                try:
                                    pub_date_obj = datetime.strptime(pub_date, date_format)
                                    date = pub_date_obj.strftime("%mæœˆ%dæ—¥")
                                    break
                                except:
                                    continue
                        except:
                            pass
                    
                    articles.append({
                        "title": title.strip(),
                        "link": link,
                        "date": date,
                        "source": source_name
                    })
        
        print(f"[DEBUG] {source_name}: æˆåŠŸæå– {len(articles)} ç¯‡AIç›¸å…³æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"[ERROR] è·å–APIæ–°é—»æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return []

def get_ai_news():
    """ä»å¤šä¸ªæ•°æ®æºè·å–AIæ–°é—»ï¼ŒæŒ‰ç…§ä¼˜å…ˆçº§æ’åº"""
    print("[INFO] å¼€å§‹è·å–AIæ–°é—»...")
    all_articles = []
    
    # ä»é…ç½®æ–‡ä»¶è·å–å¯ç”¨çš„æ•°æ®æºï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
    enabled_sources = [source for source in NEWS_SOURCES if source.get("enabled", True)]
    enabled_sources.sort(key=lambda x: x.get("priority", 3), reverse=True)
    print(f"[INFO] å¯ç”¨çš„æ–°é—»æºæ•°é‡: {len(enabled_sources)}")
    
    # ä»æ¯ä¸ªæ•°æ®æºè·å–æ–°é—»
    for source in enabled_sources:
        print(f"[INFO] æ­£åœ¨ä» {source['name']} (ä¼˜å…ˆçº§: {source.get('priority', 3)}) è·å–æ–°é—»...")
        articles = get_ai_news_from_source(source["url"], source["name"])
        
        # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ ä¼˜å…ˆçº§ä¿¡æ¯
        priority = source.get("priority", 3)
        for article in articles:
            article["priority"] = priority
        
        # é™åˆ¶æ¯ä¸ªæºçš„æ–‡ç« æ•°é‡
        articles = articles[:MAX_ARTICLES_PER_SOURCE]
        all_articles.extend(articles)
        print(f"[INFO] ä» {source['name']} è·å–äº† {len(articles)} ç¯‡æ–‡ç« ")
    
    print(f"[INFO] æ€»å…±è·å–äº† {len(all_articles)} ç¯‡æ–‡ç« ï¼ˆåŒ…å«é‡å¤ï¼‰")
    
    # å»é‡ - ä¼˜å…ˆåŸºäºæ ‡é¢˜å»é‡ï¼Œé¿å…é‡å¤å†…å®¹
    seen_titles = set()
    unique_articles = []
    for article in all_articles:
        # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        clean_title = article["title"].strip()
        # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå»é‡çš„ä¸»è¦ä¾æ®
        if clean_title not in seen_titles and len(clean_title) > 10:
            seen_titles.add(clean_title)
            unique_articles.append(article)
    
    print(f"[INFO] å»é‡åå‰©ä½™ {len(unique_articles)} ç¯‡æ–‡ç« ")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºæ–‡ç« 
    unique_articles.sort(key=lambda x: x.get("priority", 3), reverse=True)
    
    # æ ¹æ®ä¼˜å…ˆçº§é™åˆ¶æ–‡ç« æ•°é‡
    priority_articles = []
    priority_counts = {5: 0, 4: 0, 3: 0, 2: 0, 1: 0}
    
    for article in unique_articles:
        priority = article.get("priority", 3)
        if priority_counts[priority] < MAX_ARTICLES_PER_PRIORITY.get(priority, 4):
            priority_articles.append(article)
            priority_counts[priority] += 1
        
        # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œæå‰é€€å‡º
        if len(priority_articles) >= MAX_TOTAL_ARTICLES:
            break
    
    print(f"[INFO] æœ€ç»ˆç­›é€‰åä¿ç•™ {len(priority_articles)} ç¯‡æ–‡ç« ")
    print(f"[DEBUG] å„ä¼˜å…ˆçº§æ–‡ç« æ•°é‡: {priority_counts}")
    
    return priority_articles

def summarize_news(news_list):
    """ä½¿ç”¨AIå¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‘˜è¦"""
    try:
        print(f"[INFO] å¼€å§‹ç”Ÿæˆæ–°é—»æ‘˜è¦ï¼Œå…± {len(news_list)} ç¯‡æ–‡ç« ...")
        
        # æ„å»ºåŒ…å«æ¥æºä¿¡æ¯çš„æ–°é—»æ–‡æœ¬
        news_items = []
        for news in news_list:
            if "source" in news:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ¥æº: {news["source"]}, æ—¥æœŸ: {news["date"]})')
            else:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ—¥æœŸ: {news["date"]})')
        news_text = "\n".join(news_items)
        
        print(f"[DEBUG] å‘é€ç»™AIçš„æ–°é—»æ–‡æœ¬é•¿åº¦: {len(news_text)} å­—ç¬¦")
        print(f"[DEBUG] AI APIåœ°å€: {client.base_url}")
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹AIæ–°é—»ï¼Œæå–å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿:\n{news_text}"}
            ],
            max_tokens=1000
        )
        print(f"[DEBUG] AI APIå“åº”çŠ¶æ€: æˆåŠŸ")
        print(f"[DEBUG] AI APIå“åº”å†…å®¹é•¿åº¦: {len(response.choices[0].message.content)} å­—ç¬¦")
        
        summary = response.choices[0].message.content.strip()
        print(f"[INFO] æ–°é—»æ‘˜è¦ç”ŸæˆæˆåŠŸ")
        return summary
    except Exception as e:
        print(f"[ERROR] ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

def send_to_feishu(webhook_url, summary, news_list, image_key=None, webhook_name="æœªå‘½å"):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤èŠï¼Œæ”¯æŒå¡ç‰‡æ¶ˆæ¯æ ¼å¼"""
    try:
        print(f"[INFO] å¼€å§‹å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ [{webhook_name}]...")
        print(f"[DEBUG] é£ä¹¦Webhook URL: {webhook_url[:50]}...")
        print(f"[DEBUG] æ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
        print(f"[DEBUG] æ–°é—»æ•°é‡: {len(news_list)}")
        
        headers = {
            "Content-Type": "application/json"
        }
        
        # å¤„ç†æ‘˜è¦å†…å®¹ï¼Œä¿ç•™å¿…è¦çš„æ ¼å¼æ ‡è®°
        clean_summary = summary.replace('### **å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**', '**å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**')  # ç‰¹æ®Šå¤„ç†ä¸»æ ‡é¢˜
        clean_summary = clean_summary.replace('### **æ ¸å¿ƒè¶‹åŠ¿æç‚¼**', '**æ ¸å¿ƒè¶‹åŠ¿æç‚¼**')  # ç‰¹æ®Šå¤„ç†æ ¸å¿ƒè¶‹åŠ¿æ ‡é¢˜
        clean_summary = clean_summary.replace('### ', '')  # ç§»é™¤å…¶ä»–ä¸»æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#### ', '')  # ç§»é™¤å­æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#', '')  # ç§»é™¤äº•å·æ ‡è®°
        clean_summary = clean_summary.replace('`', '')  # ç§»é™¤åå¼•å·æ ‡è®°
        clean_summary = clean_summary.replace('_', '')  # ç§»é™¤ä¸‹åˆ’çº¿æ ‡è®°
        clean_summary = clean_summary.replace('~', '')  # ç§»é™¤æ³¢æµªçº¿æ ‡è®°
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯å†…å®¹
        card_elements = []
        
        # ç§»é™¤äº†å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        # ç§»é™¤äº†é‡å¤çš„æ—¥æœŸæ˜¾ç¤ºï¼Œé¿å…ä¸å¡ç‰‡å¤´éƒ¨æ ‡é¢˜é‡å¤
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ‘˜è¦æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“Š ä»Šæ—¥AIçƒ­ç‚¹æ‘˜è¦:**"
            }
        })
        
        # æ·»åŠ æ‘˜è¦å†…å®¹ï¼ˆä¼˜åŒ–æ ‡é¢˜å’Œæ­£æ–‡çš„æ ¼å¼å¤„ç†ï¼‰
        # æŒ‰æ®µè½åˆ†å‰²
        summary_paragraphs = clean_summary.split('\n\n')
        for paragraph in summary_paragraphs:
            if paragraph.strip():
                # å¤„ç†æ®µè½ä¸­çš„æ¢è¡Œ
                lines = paragraph.split('\n')
                formatted_lines = []
                for line in lines:
                    if line.strip():
                        # ä¿ç•™åŸæœ‰çš„ç²—ä½“æ ‡è®°
                        formatted_lines.append(line.strip())
                
                formatted_paragraph = '\n'.join(formatted_lines)
                
                # è¯†åˆ«å¹¶å¤„ç†æ ‡é¢˜
                stripped_paragraph = formatted_paragraph.strip()
                if stripped_paragraph.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) and len(stripped_paragraph) < 100:
                    # æ•°å­—å¼€å¤´çš„çŸ­æ®µè½ï¼Œè®¤ä¸ºæ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif stripped_paragraph.startswith("å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“") or stripped_paragraph.startswith("æ ¸å¿ƒè¶‹åŠ¿æ€»ç»“"):
                    # ç‰¹æ®Šæ ‡é¢˜å¤„ç†
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif len(stripped_paragraph) < 50 and ':' in stripped_paragraph and stripped_paragraph.count(':') <= 1:
                    # çŸ­æ®µè½ä¸”åŒ…å«å•ä¸ªå†’å·ï¼Œå¯èƒ½æ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                
                card_elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": formatted_paragraph
                    }
                })
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ–°é—»åˆ—è¡¨æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° è¯¦ç»†æ–°é—»åˆ—è¡¨:**"
            }
        })
        
        # æ·»åŠ æ–°é—»æ¡ç›®
        for i, news in enumerate(news_list[:10], 1):  # é™åˆ¶æœ€å¤šæ˜¾ç¤º10æ¡æ–°é—»
            # æ„å»ºæ–°é—»æ¡ç›®å†…å®¹ï¼ŒåŒ…å«æ¥æºä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "source" in news:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ¥æº: {news['source']} æ—¥æœŸ: {news['date']}"
            else:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ—¥æœŸ: {news['date']}"
                
            card_elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": news_content
                }
            })
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯
        data = {
            "msg_type": "interactive",
            "card": {
                "config": {
                    "wide_screen_mode": True,
                    "enable_forward": True
                },
                "header": {
                    "template": "blue",
                    "title": {
                        "content": "ğŸ¤– AIæ—¥æŠ¥ - {}".format(datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")),
                        "tag": "plain_text"
                    }
                },
                "elements": card_elements
            }
        }
            
        print(f"[DEBUG] æ„å»ºçš„é£ä¹¦æ¶ˆæ¯æ•°æ®å¤§å°: {len(json.dumps(data, ensure_ascii=False).encode('utf-8'))} å­—èŠ‚")
        
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data, ensure_ascii=False).encode('utf-8'))
        print(f"[DEBUG] é£ä¹¦APIå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"[DEBUG] é£ä¹¦APIå“åº”å†…å®¹: {response.text}")
        
        response.raise_for_status()
        print(f"[INFO] æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦ [{webhook_name}]")
        return True
    except Exception as e:
        print(f"[ERROR] å‘é€åˆ°é£ä¹¦ [{webhook_name}] æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False

def print_webhook_configs():
    """æ‰“å°å½“å‰Webhooké…ç½®ä¿¡æ¯"""
    print("[INFO] å½“å‰Webhooké…ç½®:")
    for i, config in enumerate(WEBHOOK_CONFIGS, 1):
        status = "å¯ç”¨" if config.get("enabled", True) else "ç¦ç”¨"
        image_support = "æ”¯æŒ" if config.get("send_image", False) else "ä¸æ”¯æŒ"
        print(f"  {i}. [{config['name']}] - çŠ¶æ€: {status}, å›¾ç‰‡: {image_support}")
        print(f"     URL: {config['url'][:50]}...")

def send_to_multiple_webhooks(summary, news_list, image_key=None):
    """å‘é€æ¶ˆæ¯åˆ°å¤šä¸ªé£ä¹¦ç¾¤èŠ"""
    print(f"[INFO] å¼€å§‹å‘é€æ¶ˆæ¯åˆ°å¤šä¸ªWebhookï¼Œå…± {len(WEBHOOK_CONFIGS)} ä¸ªé…ç½®")
    
    success_count = 0
    failed_count = 0
    
    for config in WEBHOOK_CONFIGS:
        if not config.get("enabled", True):
            print(f"[INFO] è·³è¿‡å·²ç¦ç”¨çš„Webhook [{config['name']}]")
            continue
            
        webhook_name = config["name"]
        webhook_url = config["url"]
        send_image = config.get("send_image", False)
        
        print(f"[INFO] æ­£åœ¨å‘é€åˆ°Webhook [{webhook_name}]...")
        
        try:
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å‘é€å›¾ç‰‡
            current_image_key = image_key if send_image else None
            
            success = send_to_feishu(webhook_url, summary, news_list, current_image_key, webhook_name)
            
            if success:
                success_count += 1
                print(f"[SUCCESS] æˆåŠŸå‘é€åˆ° [{webhook_name}]")
            else:
                failed_count += 1
                print(f"[ERROR] å‘é€åˆ° [{webhook_name}] å¤±è´¥")
                
        except Exception as e:
            failed_count += 1
            print(f"[ERROR] å‘é€åˆ° [{webhook_name}] æ—¶å‡ºç°å¼‚å¸¸: {e}")
    
    print(f"[INFO] å¤šWebhookå‘é€å®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
    return success_count, failed_count

def upload_image_to_feishu(image_path, access_token):
    """ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦å¹¶è·å–image_key"""
    try:
        upload_url = "https://open.feishu.cn/open-apis/im/v1/images"
        
        headers = {
            "Authorization": f"Bearer {access_token}",
        }
        
        files = {
            "image_type": (None, "message"),
            "image": (os.path.basename(image_path), open(image_path, "rb"), "image/jpeg") 
        }
        
        response = requests.post(upload_url, headers=headers, files=files)
        response.raise_for_status()
        
        result = response.json()
        if result.get("code") == 0:
            image_key = result["data"]["image_key"]
            print(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œimage_key: {image_key}")
            return image_key
        else:
            print("å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•° - ç«‹å³æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡"""
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    global logger
    logger = setup_logging()
    
    if logger:
        logger.info("=" * 60)
        logger.info(f"AIæ—¥æŠ¥æœºå™¨äººå¯åŠ¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
    else:
        print("=" * 60)
        print(f"AIæ—¥æŠ¥æœºå™¨äººå¯åŠ¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
    
    try:
        # æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡
        if logger:
            logger.info("å¼€å§‹æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡...")
        else:
            print("å¼€å§‹æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡...")
        
        success = execute_ai_robot_task()
        
        if success:
            if logger:
                logger.info("AIæ—¥æŠ¥ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
            else:
                print("AIæ—¥æŠ¥ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        else:
            if logger:
                logger.error("AIæ—¥æŠ¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            else:
                print("AIæ—¥æŠ¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
    
    except Exception as e:
        if logger:
            logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        else:
            print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    if logger:
        logger.info("=" * 60)
        logger.info(f"AIæ—¥æŠ¥æœºå™¨äººç»“æŸ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
    else:
        print("=" * 60)
        print(f"AIæ—¥æŠ¥æœºå™¨äººç»“æŸ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

if __name__ == "__main__":
    main()