import requests
from bs4 import BeautifulSoup
import os
from openai import OpenAI
import json
from datetime import datetime
import re
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import feedparser
from config import NEWS_SOURCES, MAX_ARTICLES_PER_SOURCE, MAX_TOTAL_ARTICLES, MAX_ARTICLES_PER_PRIORITY, AI_KEYWORDS, REQUEST_TIMEOUT, RETRY_COUNT, RETRY_DELAY

# é…ç½®OpenAI API
# è¯·å°†ä¸‹é¢çš„"your_actual_api_key_here"æ›¿æ¢ä¸ºæ‚¨ä»deepseekè·å–çš„å®é™…APIå¯†é’¥
API_KEY = "sk-e9c92f4884e742c1a533d17c1ab729d0"
client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com/")

def get_tenant_access_token(app_id, app_secret):
    """è·å–é£ä¹¦åº”ç”¨çš„tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            token = result["tenant_access_token"]
            print("æˆåŠŸè·å–tenant_access_token")
            return token
        else:
            print("è·å–tenant_access_tokenå¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"è·å–tenant_access_tokenæ—¶å‡ºé”™: {e}")
        return None

def get_ai_news_from_source(url, source_name="æœºå™¨ä¹‹å¿ƒ"):
    """ä»æŒ‡å®šURLè·å–AIæ–°é—»"""
    try:
        print(f"æ­£åœ¨ä» {source_name} è·å–æ–°é—»...")
        
        # æ ¹æ®ä¸åŒçš„æ•°æ®æºä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
        if source_name == "æœºå™¨ä¹‹å¿ƒ":
            return get_jiqizhixin_news(url, source_name)
        elif source_name == "36æ°ª":
            return get_36kr_news(url, source_name)
        elif source_name == "InfoQ":
            return get_infoq_news(url, source_name)
        elif source_name == "AMiner":
            return get_aminer_news(url, source_name)
        elif source_name == "é›·é”‹ç½‘":
            return get_leiphone_news(url, source_name)
        elif source_name == "VentureBeat":
            return get_venturebeat_news(url, source_name)
        elif source_name == "TechCrunch":
            return get_techcrunch_news(url, source_name)
        elif source_name.endswith("RSS") or "rss" in source_name.lower():
            return get_rss_news(url, source_name)
        elif source_name.endswith("API") or "api" in source_name.lower():
            return get_api_news(url, source_name)
        else:
            return get_generic_news(url, source_name)
    except Exception as e:
        print(f"ä» {source_name} è·å–æ–°é—»æ—¶å‡ºé”™ {url}: {e}")
        return []

def get_jiqizhixin_news(url, source_name):
    """è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "home__left-body")))
        
        page_source = driver.page_source
        driver.quit()
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_="body-title")
        
        if len(news_items) < 5:
            more_items = soup.find_all("a", class_=["article-item", "news-item", "post-title", "title"])
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            potential_titles = soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"title|heading|headline"))
            more_items = soup.find_all("a", href=re.compile(r"articles|news|post|reference"))
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            ai_links = []
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and ("ai" in text.lower() or "æœºå™¨" in text or "æ™ºèƒ½" in text or "AI" in text):
                    ai_links.append(link)
            news_items.extend(ai_links)
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.jiqizhixin.com" + link
                    else:
                        link = "https://www.jiqizhixin.com/" + link
            else:
                onclick = item.get("onclick", "")
                if onclick:
                    match = re.search(r"'(https?://[^']+)'", onclick)
                    if match:
                        link = match.group(1)
                    else:
                        match = re.search(r"'(/articles/[^']+)'", onclick)
                        if match:
                            link = "https://www.jiqizhixin.com" + match.group(1)
                if not link:
                    data_href = item.get("data-href", "")
                    if data_href:
                        if not data_href.startswith("http"):
                            if data_href.startswith("/"):
                                link = "https://www.jiqizhixin.com" + data_href
                            else:
                                link = "https://www.jiqizhixin.com/" + data_href
                        else:
                            link = data_href
            
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            parent = item.find_parent()
            if parent:
                date_elements = parent.find_all(string=re.compile(r"\d{1,2}æœˆ\d{1,2}æ—¥"))
                if date_elements:
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", str(date_elements[0]))
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_36kr_news(url, source_name):
    """è·å–36æ°ªAIæ–°é—»"""
    try:
        # å…ˆå°è¯•ä½¿ç”¨requestsè·å–
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„ç±»åå’Œé€‰æ‹©å™¨
        news_items = []
        
        # æ–¹æ³•1ï¼šå¯»æ‰¾æ–‡ç« é“¾æ¥
        news_items.extend(soup.find_all("a", class_=["item-title", "article-title", "title", "post-title"]))
        
        # æ–¹æ³•2ï¼šå¯»æ‰¾åŒ…å«AIå…³é”®è¯çš„é“¾æ¥
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and any(keyword in text.lower() for keyword in AI_KEYWORDS):
                    news_items.append(link)
        
        # æ–¹æ³•3ï¼šå¯»æ‰¾ç‰¹å®šçš„æ–‡ç« å®¹å™¨
        if len(news_items) < 5:
            news_items.extend(soup.find_all("div", class_=re.compile(r"item|article|post")))
        
        for item in news_items:
            # å¦‚æœæ˜¯divå®¹å™¨ï¼Œéœ€è¦ä»ä¸­æå–é“¾æ¥
            if item.name == "div":
                link_element = item.find("a")
                if not link_element:
                    continue
                title = link_element.get_text(strip=True)
                link = link_element.get("href", "")
            else:
                title = item.get_text(strip=True)
                link = item.get("href", "")
            
            if len(title) < 5:
                continue
                
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://36kr.com" + link
                    else:
                        link = "https://36kr.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ—¥æœŸ
            parent = item.find_parent()
            if parent:
                time_element = parent.find("time") or parent.find("span", class_=re.compile(r"time|date"))
                if time_element:
                    date_text = time_element.get_text(strip=True)
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", date_text)
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–36æ°ªæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_infoq_news(url, source_name):
    """è·å–InfoQ AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["news-title", "article-title", "title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.infoq.cn" + link
                    else:
                        link = "https://www.infoq.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–InfoQæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_aminer_news(url, source_name):
    """è·å–AMiner AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "paper-title", "article-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.aminer.cn" + link
                    else:
                        link = "https://www.aminer.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–AMineræ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_leiphone_news(url, source_name):
    """è·å–é›·é”‹ç½‘AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.leiphone.com" + link
                    else:
                        link = "https://www.leiphone.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–é›·é”‹ç½‘æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_venturebeat_news(url, source_name):
    """è·å–VentureBeat AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://venturebeat.com" + link
                else:
                    link = "https://venturebeat.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–VentureBeatæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_techcrunch_news(url, source_name):
    """è·å–TechCrunch AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://techcrunch.com" + link
                else:
                    link = "https://techcrunch.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–TechCrunchæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_generic_news(url, source_name):
    """é€šç”¨æ–°é—»è·å–æ–¹æ³•"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        
        wait = WebDriverWait(driver, 10)
        
        page_source = driver.page_source
        driver.quit()
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        print(f"è·å–é€šç”¨æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_rss_news(url, source_name):
    """è·å–RSS/ATOMæ–°é—»"""
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries:
            title = entry.get('title', '')
            if not title or len(title.strip()) < 5:
                continue
                
            link = entry.get('link', '')
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            title_lower = title.lower()
            ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                          "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                          "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
            
            if not any(keyword in title_lower for keyword in ai_keywords):
                continue
            
            # è·å–æ—¥æœŸ
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            if 'published' in entry:
                try:
                    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z')
                    date = pub_date.strftime("%mæœˆ%dæ—¥")
                except:
                    pass
            
            articles.append({
                "title": title.strip(),
                "link": link,
                "date": date,
                "source": source_name
            })
        
        return articles
    except Exception as e:
        print(f"è·å–RSSæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_api_news(url, source_name):
    """è·å–APIæ–°é—»"""
    try:
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        articles = []
        
        # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
        if isinstance(data, dict):
            # å°è¯•ä¸åŒçš„APIå“åº”æ ¼å¼
            articles_data = []
            
            # æ ¼å¼1: {articles: [...]}
            if 'articles' in data:
                articles_data = data['articles']
            # æ ¼å¼2: {data: [...]}
            elif 'data' in data and isinstance(data['data'], list):
                articles_data = data['data']
            # æ ¼å¼3: {items: [...]}
            elif 'items' in data:
                articles_data = data['items']
            # æ ¼å¼4: {results: [...]}
            elif 'results' in data:
                articles_data = data['results']
            # æ ¼å¼5: {response: {docs: [...]}}
            elif 'response' in data and isinstance(data['response'], dict) and 'docs' in data['response']:
                articles_data = data['response']['docs']
            
            for item in articles_data:
                if isinstance(item, dict):
                    title = item.get('title', item.get('headline', item.get('name', '')))
                    link = item.get('url', item.get('link', item.get('href', '')))
                    
                    if not title or len(title.strip()) < 5:
                        continue
                    
                    if not link:
                        continue
                    
                    # ç­›é€‰AIç›¸å…³æ–°é—»
                    title_lower = title.lower()
                    ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                                  "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                                  "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
                    
                    if not any(keyword in title_lower for keyword in ai_keywords):
                        continue
                    
                    # è·å–æ—¥æœŸ
                    date = datetime.now().strftime("%mæœˆ%dæ—¥")
                    pub_date = item.get('publishedAt', item.get('published_date', item.get('date', '')))
                    if pub_date:
                        try:
                            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
                            for date_format in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d %H:%M:%S']:
                                try:
                                    pub_date_obj = datetime.strptime(pub_date, date_format)
                                    date = pub_date_obj.strftime("%mæœˆ%dæ—¥")
                                    break
                                except:
                                    continue
                        except:
                            pass
                    
                    articles.append({
                        "title": title.strip(),
                        "link": link,
                        "date": date,
                        "source": source_name
                    })
        
        return articles
    except Exception as e:
        print(f"è·å–APIæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_ai_news():
    """ä»å¤šä¸ªæ•°æ®æºè·å–AIæ–°é—»ï¼ŒæŒ‰ç…§ä¼˜å…ˆçº§æ’åº"""
    all_articles = []
    
    # ä»é…ç½®æ–‡ä»¶è·å–å¯ç”¨çš„æ•°æ®æºï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
    enabled_sources = [source for source in NEWS_SOURCES if source.get("enabled", True)]
    enabled_sources.sort(key=lambda x: x.get("priority", 3), reverse=True)
    
    # ä»æ¯ä¸ªæ•°æ®æºè·å–æ–°é—»
    for source in enabled_sources:
        print(f"æ­£åœ¨ä» {source['name']} (ä¼˜å…ˆçº§: {source.get('priority', 3)}) è·å–æ–°é—»...")
        articles = get_ai_news_from_source(source["url"], source["name"])
        
        # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ ä¼˜å…ˆçº§ä¿¡æ¯
        priority = source.get("priority", 3)
        for article in articles:
            article["priority"] = priority
        
        # é™åˆ¶æ¯ä¸ªæºçš„æ–‡ç« æ•°é‡
        articles = articles[:MAX_ARTICLES_PER_SOURCE]
        all_articles.extend(articles)
    
    # å»é‡ - ä¼˜å…ˆåŸºäºæ ‡é¢˜å»é‡ï¼Œé¿å…é‡å¤å†…å®¹
    seen_titles = set()
    unique_articles = []
    for article in all_articles:
        # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        clean_title = article["title"].strip()
        # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå»é‡çš„ä¸»è¦ä¾æ®
        if clean_title not in seen_titles and len(clean_title) > 10:
            seen_titles.add(clean_title)
            unique_articles.append(article)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºæ–‡ç« 
    unique_articles.sort(key=lambda x: x.get("priority", 3), reverse=True)
    
    # æ ¹æ®ä¼˜å…ˆçº§é™åˆ¶æ–‡ç« æ•°é‡
    priority_articles = []
    priority_counts = {5: 0, 4: 0, 3: 0, 2: 0, 1: 0}
    
    for article in unique_articles:
        priority = article.get("priority", 3)
        if priority_counts[priority] < MAX_ARTICLES_PER_PRIORITY.get(priority, 4):
            priority_articles.append(article)
            priority_counts[priority] += 1
        
        # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œæå‰é€€å‡º
        if len(priority_articles) >= MAX_TOTAL_ARTICLES:
            break
    
    return priority_articles

def summarize_news(news_list):
    """ä½¿ç”¨AIå¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‘˜è¦"""
    try:
        # æ„å»ºåŒ…å«æ¥æºä¿¡æ¯çš„æ–°é—»æ–‡æœ¬
        news_items = []
        for news in news_list:
            if "source" in news:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ¥æº: {news["source"]}, æ—¥æœŸ: {news["date"]})')
            else:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ—¥æœŸ: {news["date"]})')
        news_text = "\n".join(news_items)
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹AIæ–°é—»ï¼Œæå–å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿:\n{news_text}"}
            ],
            max_tokens=1000
        )
        print(f"APIå“åº”: {response}") # Debugging line
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

def send_to_feishu(webhook_url, summary, news_list, image_key=None):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤èŠï¼Œæ”¯æŒå¡ç‰‡æ¶ˆæ¯æ ¼å¼"""
    try:
        headers = {
            "Content-Type": "application/json"
        }
        
        # å¤„ç†æ‘˜è¦å†…å®¹ï¼Œä¿ç•™å¿…è¦çš„æ ¼å¼æ ‡è®°
        clean_summary = summary.replace('### **å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**', '**å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**')  # ç‰¹æ®Šå¤„ç†ä¸»æ ‡é¢˜
        clean_summary = clean_summary.replace('### **æ ¸å¿ƒè¶‹åŠ¿æç‚¼**', '**æ ¸å¿ƒè¶‹åŠ¿æç‚¼**')  # ç‰¹æ®Šå¤„ç†æ ¸å¿ƒè¶‹åŠ¿æ ‡é¢˜
        clean_summary = clean_summary.replace('### ', '')  # ç§»é™¤å…¶ä»–ä¸»æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#### ', '')  # ç§»é™¤å­æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#', '')  # ç§»é™¤äº•å·æ ‡è®°
        clean_summary = clean_summary.replace('`', '')  # ç§»é™¤åå¼•å·æ ‡è®°
        clean_summary = clean_summary.replace('_', '')  # ç§»é™¤ä¸‹åˆ’çº¿æ ‡è®°
        clean_summary = clean_summary.replace('~', '')  # ç§»é™¤æ³¢æµªçº¿æ ‡è®°
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯å†…å®¹
        card_elements = []
        
        # ç§»é™¤äº†å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        # ç§»é™¤äº†é‡å¤çš„æ—¥æœŸæ˜¾ç¤ºï¼Œé¿å…ä¸å¡ç‰‡å¤´éƒ¨æ ‡é¢˜é‡å¤
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ‘˜è¦æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“Š ä»Šæ—¥AIçƒ­ç‚¹æ‘˜è¦:**"
            }
        })
        
        # æ·»åŠ æ‘˜è¦å†…å®¹ï¼ˆä¼˜åŒ–æ ‡é¢˜å’Œæ­£æ–‡çš„æ ¼å¼å¤„ç†ï¼‰
        # æŒ‰æ®µè½åˆ†å‰²
        summary_paragraphs = clean_summary.split('\n\n')
        for paragraph in summary_paragraphs:
            if paragraph.strip():
                # å¤„ç†æ®µè½ä¸­çš„æ¢è¡Œ
                lines = paragraph.split('\n')
                formatted_lines = []
                for line in lines:
                    if line.strip():
                        # ä¿ç•™åŸæœ‰çš„ç²—ä½“æ ‡è®°
                        formatted_lines.append(line.strip())
                
                formatted_paragraph = '\n'.join(formatted_lines)
                
                # è¯†åˆ«å¹¶å¤„ç†æ ‡é¢˜
                stripped_paragraph = formatted_paragraph.strip()
                if stripped_paragraph.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) and len(stripped_paragraph) < 100:
                    # æ•°å­—å¼€å¤´çš„çŸ­æ®µè½ï¼Œè®¤ä¸ºæ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif stripped_paragraph.startswith("å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“") or stripped_paragraph.startswith("æ ¸å¿ƒè¶‹åŠ¿æ€»ç»“"):
                    # ç‰¹æ®Šæ ‡é¢˜å¤„ç†
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif len(stripped_paragraph) < 50 and ':' in stripped_paragraph and stripped_paragraph.count(':') <= 1:
                    # çŸ­æ®µè½ä¸”åŒ…å«å•ä¸ªå†’å·ï¼Œå¯èƒ½æ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                
                card_elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": formatted_paragraph
                    }
                })
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ–°é—»åˆ—è¡¨æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° è¯¦ç»†æ–°é—»åˆ—è¡¨:**"
            }
        })
        
        # æ·»åŠ æ–°é—»æ¡ç›®
        for i, news in enumerate(news_list[:10], 1):  # é™åˆ¶æœ€å¤šæ˜¾ç¤º10æ¡æ–°é—»
            # æ„å»ºæ–°é—»æ¡ç›®å†…å®¹ï¼ŒåŒ…å«æ¥æºä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "source" in news:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ¥æº: {news['source']} æ—¥æœŸ: {news['date']}"
            else:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ—¥æœŸ: {news['date']}"
                
            card_elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": news_content
                }
            })
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯
        data = {
            "msg_type": "interactive",
            "card": {
                "config": {
                    "wide_screen_mode": True,
                    "enable_forward": True
                },
                "header": {
                    "template": "blue",
                    "title": {
                        "content": "ğŸ¤– AIæ—¥æŠ¥ - {}".format(datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")),
                        "tag": "plain_text"
                    }
                },
                "elements": card_elements
            }
        }
            
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data, ensure_ascii=False).encode('utf-8'))
        response.raise_for_status()
        print("æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
        return True
    except Exception as e:
        print(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return False

def upload_image_to_feishu(image_path, access_token):
    """ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦å¹¶è·å–image_key"""
    try:
        upload_url = "https://open.feishu.cn/open-apis/im/v1/images"
        
        headers = {
            "Authorization": f"Bearer {access_token}",
        }
        
        files = {
            "image_type": (None, "message"),
            "image": (os.path.basename(image_path), open(image_path, "rb"), "image/jpeg") 
        }
        
        response = requests.post(upload_url, headers=headers, files=files)
        response.raise_for_status()
        
        result = response.json()
        if result.get("code") == 0:
            image_key = result["data"]["image_key"]
            print(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œimage_key: {image_key}")
            return image_key
        else:
            print("å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡ - {}".format(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))
    
    # é£ä¹¦åº”ç”¨å‡­è¯
    feishu_app_id = "cli_a8ef4e27bd85900b"
    feishu_app_secret = "By4Y7Z2NpQvovyJ0Efp2CgyOF8dAC7bV"
    
    # è·å–tenant_access_token
    access_token = get_tenant_access_token(feishu_app_id, feishu_app_secret)
    if not access_token:
        print("æ— æ³•è·å–access_tokenï¼Œé€€å‡ºä»»åŠ¡ã€‚")
        return

    # è·å–AIæ–°é—»ï¼ˆä»å¤šä¸ªæ•°æ®æºï¼‰
    print("æ­£åœ¨è·å–AIæ–°é—»...")
    ai_news = get_ai_news()
    
    if ai_news:
        print(f"æˆåŠŸè·å– {len(ai_news)} æ¡æ–°é—»")
        
        # ç”Ÿæˆæ‘˜è¦
        print("æ­£åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦...")
        summary = summarize_news(ai_news)
        
        print("ç”Ÿæˆçš„æ—¥æŠ¥æ‘˜è¦:")
        print(summary)
        
        # é£ä¹¦webhook URL
        feishu_webhook = "https://open.feishu.cn/open-apis/bot/v2/hook/23b17757-5370-4f6c-92ab-7625569ca7a7"
        
        # ä¸Šä¼ ä¸»é¢˜å›¾ç‰‡å¹¶å‘é€
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡å›¾ç‰‡ä¸Šä¼ 
        image_path = "/home/ubuntu/upload/search_images/QkPqdKuxZOlT.jpg" # é€‰æ‹©ä¸€å¼ AIç›¸å…³çš„å›¾ç‰‡
        image_key = None
        if os.path.exists(image_path):
            image_key = upload_image_to_feishu(image_path, access_token)
        else:
            print(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}ï¼Œå·²ç§»é™¤å›¾ç‰‡ä»¥ç¡®ä¿æ¶ˆæ¯å‘é€æˆåŠŸ")
            # å·²ç§»é™¤å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        if image_key:
            send_to_feishu(feishu_webhook, summary, ai_news, image_key)
        else:
            send_to_feishu(feishu_webhook, summary, ai_news) # å¦‚æœå›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œåˆ™åªå‘é€æ–‡æœ¬
        
    else:
        print("æœªèƒ½è·å–AIæ–°é—»")

if __name__ == "__main__":
    main()

