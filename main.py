import requests
from bs4 import BeautifulSoup
import os
from openai import OpenAI
import json
from datetime import datetime
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# é…ç½®OpenAI API
# è¯·å°†ä¸‹é¢çš„"your_actual_api_key_here"æ›¿æ¢ä¸ºæ‚¨ä»deepseekè·å–çš„å®é™…APIå¯†é’¥
API_KEY = "sk-e9c92f4884e742c1a533d17c1ab729d0"
client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com/")

def get_tenant_access_token(app_id, app_secret):
    """è·å–é£ä¹¦åº”ç”¨çš„tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            token = result["tenant_access_token"]
            print("æˆåŠŸè·å–tenant_access_token")
            return token
        else:
            print("è·å–tenant_access_tokenå¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"è·å–tenant_access_tokenæ—¶å‡ºé”™: {e}")
        return None

def get_ai_news(url):
    """ä»æŒ‡å®šURLè·å–AIæ–°é—»"""
    try:
        # è®¾ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        # åˆå§‹åŒ–WebDriver
        from webdriver_manager.chrome import ChromeDriverManager
        from selenium.webdriver.chrome.service import Service
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        wait = WebDriverWait(driver, 10)
        
        # ç­‰å¾…æ–°é—»å†…å®¹åŠ è½½å®Œæˆ
        # æ ¹æ®ç½‘ç«™çš„å®é™…ç»“æ„ï¼Œç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "home__left-body")))
        
        # è·å–é¡µé¢æºç 
        page_source = driver.page_source
        driver.quit()
        
        # ä½¿ç”¨BeautifulSoupè§£æé¡µé¢
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        # æŸ¥æ‰¾æ–°é—»æ¡ç›® - å°è¯•å¤šç§é€‰æ‹©å™¨ä»¥è·å–æ›´å¤šæ–°é—»
        # 1. é¦–å…ˆå°è¯•body-titleç±»
        news_items = soup.find_all("a", class_="body-title")
        
        # 2. å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ç±»å
        if len(news_items) < 5:
            more_items = soup.find_all("a", class_=["article-item", "news-item", "post-title", "title"])
            news_items.extend(more_items)
        
        # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
        if len(news_items) < 5:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–°é—»æ ‡é¢˜çš„å…ƒç´ 
            potential_titles = soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"title|heading|headline"))
            # æˆ–è€…æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            more_items = soup.find_all("a", href=re.compile(r"articles|news|post|reference"))
            news_items.extend(more_items)
        
        # 4. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼ŒæŸ¥æ‰¾åŒ…å«AIç›¸å…³å…³é”®è¯çš„é“¾æ¥
        if len(news_items) < 5:
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥å¹¶ç­›é€‰åŒ…å«AIç›¸å…³å…³é”®è¯çš„é“¾æ¥
            all_links = soup.find_all("a", href=True)
            ai_links = []
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and ("ai" in text.lower() or "æœºå™¨" in text or "æ™ºèƒ½" in text or "AI" in text):
                    ai_links.append(link)
            news_items.extend(ai_links)
        
        if news_items:
            for item in news_items:
                title = item.get_text(strip=True)
                # ç¡®ä¿æ ‡é¢˜æœ‰è¶³å¤Ÿçš„é•¿åº¦
                if len(title) < 5:
                    continue
                    
                link = item.get("href", "")
                # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                if link:
                    if not link.startswith("http"):
                        if link.startswith("/"):
                            link = "https://www.jiqizhixin.com" + link
                        else:
                            link = "https://www.jiqizhixin.com/" + link
                else:
                    # å¦‚æœhrefä¸ºç©ºï¼Œå°è¯•ä»onclickæˆ–å…¶ä»–å±æ€§è·å–é“¾æ¥
                    onclick = item.get("onclick", "")
                    if onclick:
                        # å°è¯•ä»onclickä¸­æå–é“¾æ¥
                        # åŒ¹é…æ›´å¹¿æ³›çš„URLæ¨¡å¼
                        match = re.search(r"'(https?://[^']+)'", onclick)
                        if match:
                            link = match.group(1)
                        else:
                            # å°è¯•åŒ¹é…ç›¸å¯¹è·¯å¾„
                            match = re.search(r"'(/articles/[^']+)'", onclick)
                            if match:
                                link = "https://www.jiqizhixin.com" + match.group(1)
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨itemçš„çˆ¶å…ƒç´ ä¸­çš„data-hrefå±æ€§æˆ–å…¶ä»–å¯èƒ½çš„å±æ€§
                    if not link:
                        data_href = item.get("data-href", "")
                        if data_href:
                            if not data_href.startswith("http"):
                                if data_href.startswith("/"):
                                    link = "https://www.jiqizhixin.com" + data_href
                                else:
                                    link = "https://www.jiqizhixin.com/" + data_href
                            else:
                                link = data_href
                
                # è·³è¿‡æ²¡æœ‰é“¾æ¥çš„æ¡ç›®
                if not link:
                    continue
                
                # å°è¯•è·å–æ—¥æœŸä¿¡æ¯
                date = datetime.now().strftime("%mæœˆ%dæ—¥")  # é»˜è®¤æ—¥æœŸ
                
                # æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ—¥æœŸä¿¡æ¯
                parent = item.find_parent()
                if parent:
                    # æŸ¥æ‰¾æ—¥æœŸå…ƒç´ 
                    date_elements = parent.find_all(string=re.compile(r"\d{1,2}æœˆ\d{1,2}æ—¥"))
                    if date_elements:
                        date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", str(date_elements[0]))
                        if date_match:
                            date = date_match.group(0)
                
                articles.append({"title": title, "link": link, "date": date})
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šçš„æ–°é—»å…ƒç´ ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ä½œä¸ºå¯èƒ½çš„æ–°é—»
            links = soup.find_all("a", href=True)
            for link in links:
                title = link.get_text(strip=True)
                href = link.get("href", "")
                # ç­›é€‰å¯èƒ½æ˜¯æ–°é—»çš„é“¾æ¥
                if title and len(title) > 10 and (href.startswith("/articles/") or "jiqizhixin.com" in href):
                    if not href.startswith("http"):
                        href = "https://www.jiqizhixin.com" + href
                    
                    # å°è¯•è·å–æ—¥æœŸä¿¡æ¯
                    date = datetime.now().strftime("%mæœˆ%dæ—¥")  # é»˜è®¤æ—¥æœŸ
                    
                    articles.append({"title": title, "link": href, "date": date})
        
        # å»é‡ - ä¼˜å…ˆåŸºäºæ ‡é¢˜å»é‡ï¼Œé¿å…é‡å¤å†…å®¹
        seen_titles = set()
        unique_articles = []
        for article in articles:
            # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            clean_title = article["title"].strip()
            # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå»é‡çš„ä¸»è¦ä¾æ®
            if clean_title not in seen_titles and len(clean_title) > 10:
                seen_titles.add(clean_title)
                unique_articles.append(article)
        
        # é™åˆ¶è¿”å›çš„æ–°é—»æ•°é‡
        return unique_articles[:15]  # è¿”å›å‰15æ¡æ–°é—»
        
    except Exception as e:
        print(f"è·å–æ–°é—»æ—¶å‡ºé”™ {url}: {e}")
        return []

def summarize_news(news_list):
    """ä½¿ç”¨AIå¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‘˜è¦"""
    try:
        news_text = "\n".join([f'æ ‡é¢˜: {news["title"]} (æ—¥æœŸ: {news["date"]})' for news in news_list])
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹AIæ–°é—»ï¼Œæå–å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿:\n{news_text}"}
            ],
            max_tokens=1000
        )
        print(f"APIå“åº”: {response}") # Debugging line
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

def send_to_feishu(webhook_url, summary, news_list, image_key=None):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤èŠï¼Œæ”¯æŒå¡ç‰‡æ¶ˆæ¯æ ¼å¼"""
    try:
        headers = {
            "Content-Type": "application/json"
        }
        
        # å¤„ç†æ‘˜è¦å†…å®¹ï¼Œä¿ç•™å¿…è¦çš„æ ¼å¼æ ‡è®°
        clean_summary = summary.replace('### **å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**', '**å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**')  # ç‰¹æ®Šå¤„ç†ä¸»æ ‡é¢˜
        clean_summary = clean_summary.replace('### **æ ¸å¿ƒè¶‹åŠ¿æç‚¼**', '**æ ¸å¿ƒè¶‹åŠ¿æç‚¼**')  # ç‰¹æ®Šå¤„ç†æ ¸å¿ƒè¶‹åŠ¿æ ‡é¢˜
        clean_summary = clean_summary.replace('### ', '')  # ç§»é™¤å…¶ä»–ä¸»æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#### ', '')  # ç§»é™¤å­æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#', '')  # ç§»é™¤äº•å·æ ‡è®°
        clean_summary = clean_summary.replace('`', '')  # ç§»é™¤åå¼•å·æ ‡è®°
        clean_summary = clean_summary.replace('_', '')  # ç§»é™¤ä¸‹åˆ’çº¿æ ‡è®°
        clean_summary = clean_summary.replace('~', '')  # ç§»é™¤æ³¢æµªçº¿æ ‡è®°
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯å†…å®¹
        card_elements = []
        
        # ç§»é™¤äº†å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        # ç§»é™¤äº†é‡å¤çš„æ—¥æœŸæ˜¾ç¤ºï¼Œé¿å…ä¸å¡ç‰‡å¤´éƒ¨æ ‡é¢˜é‡å¤
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ‘˜è¦æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“Š ä»Šæ—¥AIçƒ­ç‚¹æ‘˜è¦:**"
            }
        })
        
        # æ·»åŠ æ‘˜è¦å†…å®¹ï¼ˆä¼˜åŒ–æ ‡é¢˜å’Œæ­£æ–‡çš„æ ¼å¼å¤„ç†ï¼‰
        # æŒ‰æ®µè½åˆ†å‰²
        summary_paragraphs = clean_summary.split('\n\n')
        for paragraph in summary_paragraphs:
            if paragraph.strip():
                # å¤„ç†æ®µè½ä¸­çš„æ¢è¡Œ
                lines = paragraph.split('\n')
                formatted_lines = []
                for line in lines:
                    if line.strip():
                        # ä¿ç•™åŸæœ‰çš„ç²—ä½“æ ‡è®°
                        formatted_lines.append(line.strip())
                
                formatted_paragraph = '\n'.join(formatted_lines)
                
                # è¯†åˆ«å¹¶å¤„ç†æ ‡é¢˜
                stripped_paragraph = formatted_paragraph.strip()
                if stripped_paragraph.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) and len(stripped_paragraph) < 100:
                    # æ•°å­—å¼€å¤´çš„çŸ­æ®µè½ï¼Œè®¤ä¸ºæ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif stripped_paragraph.startswith("å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“") or stripped_paragraph.startswith("æ ¸å¿ƒè¶‹åŠ¿æ€»ç»“"):
                    # ç‰¹æ®Šæ ‡é¢˜å¤„ç†
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif len(stripped_paragraph) < 50 and ':' in stripped_paragraph and stripped_paragraph.count(':') <= 1:
                    # çŸ­æ®µè½ä¸”åŒ…å«å•ä¸ªå†’å·ï¼Œå¯èƒ½æ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                
                card_elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": formatted_paragraph
                    }
                })
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ–°é—»åˆ—è¡¨æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° è¯¦ç»†æ–°é—»åˆ—è¡¨:**"
            }
        })
        
        # æ·»åŠ æ–°é—»æ¡ç›®
        for i, news in enumerate(news_list[:10], 1):  # é™åˆ¶æœ€å¤šæ˜¾ç¤º10æ¡æ–°é—»
            card_elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"{i}. [{news['title']}]({news['link']}) æ—¥æœŸ: {news['date']}"
                }
            })
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯
        data = {
            "msg_type": "interactive",
            "card": {
                "config": {
                    "wide_screen_mode": True,
                    "enable_forward": True
                },
                "header": {
                    "template": "blue",
                    "title": {
                        "content": "ğŸ¤– AIæ—¥æŠ¥ - {}".format(datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")),
                        "tag": "plain_text"
                    }
                },
                "elements": card_elements
            }
        }
            
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data, ensure_ascii=False).encode('utf-8'))
        response.raise_for_status()
        print("æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
        return True
    except Exception as e:
        print(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return False

def upload_image_to_feishu(image_path, access_token):
    """ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦å¹¶è·å–image_key"""
    try:
        upload_url = "https://open.feishu.cn/open-apis/im/v1/images"
        
        headers = {
            "Authorization": f"Bearer {access_token}",
        }
        
        files = {
            "image_type": (None, "message"),
            "image": (os.path.basename(image_path), open(image_path, "rb"), "image/jpeg") 
        }
        
        response = requests.post(upload_url, headers=headers, files=files)
        response.raise_for_status()
        
        result = response.json()
        if result.get("code") == 0:
            image_key = result["data"]["image_key"]
            print(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œimage_key: {image_key}")
            return image_key
        else:
            print("å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {}".format(result.get("msg")))
            return None
    except Exception as e:
        print(f"ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡ - {}".format(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))
    
    # é£ä¹¦åº”ç”¨å‡­è¯
    feishu_app_id = "cli_a8ef4e27bd85900b"
    feishu_app_secret = "By4Y7Z2NpQvovyJ0Efp2CgyOF8dAC7bV"
    
    # è·å–tenant_access_token
    access_token = get_tenant_access_token(feishu_app_id, feishu_app_secret)
    if not access_token:
        print("æ— æ³•è·å–access_tokenï¼Œé€€å‡ºä»»åŠ¡ã€‚")
        return

    # æ–°é—»æºURL
    news_source_url = "https://www.jiqizhixin.com/"
    
    # è·å–AIæ–°é—»
    print("æ­£åœ¨è·å–AIæ–°é—»...")
    ai_news = get_ai_news(news_source_url)
    
    if ai_news:
        print(f"æˆåŠŸè·å– {len(ai_news)} æ¡æ–°é—»")
        
        # ç”Ÿæˆæ‘˜è¦
        print("æ­£åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦...")
        summary = summarize_news(ai_news)
        
        print("ç”Ÿæˆçš„æ—¥æŠ¥æ‘˜è¦:")
        print(summary)
        
        # é£ä¹¦webhook URL
        feishu_webhook = "https://open.feishu.cn/open-apis/bot/v2/hook/9f92c19d-9dc1-46f2-b5fa-117860a4eea5"
        
        # ä¸Šä¼ ä¸»é¢˜å›¾ç‰‡å¹¶å‘é€
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡å›¾ç‰‡ä¸Šä¼ 
        image_path = "/home/ubuntu/upload/search_images/QkPqdKuxZOlT.jpg" # é€‰æ‹©ä¸€å¼ AIç›¸å…³çš„å›¾ç‰‡
        image_key = None
        if os.path.exists(image_path):
            image_key = upload_image_to_feishu(image_path, access_token)
        else:
            print(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}ï¼Œå·²ç§»é™¤å›¾ç‰‡ä»¥ç¡®ä¿æ¶ˆæ¯å‘é€æˆåŠŸ")
            # å·²ç§»é™¤å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        if image_key:
            send_to_feishu(feishu_webhook, summary, ai_news, image_key)
        else:
            send_to_feishu(feishu_webhook, summary, ai_news) # å¦‚æœå›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œåˆ™åªå‘é€æ–‡æœ¬
        
    else:
        print("æœªèƒ½è·å–AIæ–°é—»")

if __name__ == "__main__":
    main()

