import json
import logging
import os
import re
import sys
from datetime import datetime

import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from config import (NEWS_SOURCES, MAX_ARTICLES_PER_SOURCE, MAX_TOTAL_ARTICLES, MAX_ARTICLES_PER_PRIORITY,
                    AI_KEYWORDS, REQUEST_TIMEOUT, LOG_CONFIG, MODEL_PROVIDERS, CURRENT_PROVIDER)

# PySparkæ”¯æŒ
try:
    from pyspark.sql import SparkSession
    from pyspark import SparkConf
    PYSPARK_AVAILABLE = True
except ImportError:
    PYSPARK_AVAILABLE = False
    print("PySpark not available, running in standalone mode")

# åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
def get_model_client(provider_name=None, logger=None):
    """è·å–æŒ‡å®šæ¨¡å‹ä¾›åº”å•†çš„å®¢æˆ·ç«¯"""
    if provider_name is None:
        provider_name = CURRENT_PROVIDER
    
    if provider_name not in MODEL_PROVIDERS:
        if logger:
            logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ä¾›åº”å•†: {provider_name}")
        else:
            print(f"ä¸æ”¯æŒçš„æ¨¡å‹ä¾›åº”å•†: {provider_name}")
        return None
    
    provider_config = MODEL_PROVIDERS[provider_name]
    
    if not provider_config.get("enabled", True):
        if logger:
            logger.error(f"æ¨¡å‹ä¾›åº”å•† {provider_name} æœªå¯ç”¨")
        else:
            print(f"æ¨¡å‹ä¾›åº”å•† {provider_name} æœªå¯ç”¨")
        return None
    
    try:
        if provider_name == "deepseek":
            client = OpenAI(
                api_key=provider_config["api_key"],
                base_url=provider_config["base_url"]
            )
        elif provider_name == "kimi":
            client = OpenAI(
                api_key=provider_config["api_key"],
                base_url=provider_config["base_url"]
            )
        elif provider_name == "glm":
            client = OpenAI(
                api_key=provider_config["api_key"],
                base_url=provider_config["base_url"]
            )
        else:
            if logger:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ä¾›åº”å•†: {provider_name}")
            else:
                print(f"ä¸æ”¯æŒçš„æ¨¡å‹ä¾›åº”å•†: {provider_name}")
            return None
        
        if logger:
            logger.info(f"å·²åˆå§‹åŒ– {provider_config['name']} å®¢æˆ·ç«¯")
        else:
            print(f"å·²åˆå§‹åŒ– {provider_config['name']} å®¢æˆ·ç«¯")
        
        return client
    except Exception as e:
        if logger:
            logger.error(f"åˆå§‹åŒ– {provider_config['name']} å®¢æˆ·ç«¯å¤±è´¥: {e}")
        else:
            print(f"åˆå§‹åŒ– {provider_config['name']} å®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def setup_pyspark_environment():
    """è®¾ç½®PySparkç¯å¢ƒ"""
    if not PYSPARK_AVAILABLE:
        return None
    
    try:
        # é…ç½®Spark
        conf = SparkConf()
        conf.set("spark.app.name", "AI_Daily_Robot")
        conf.set("spark.master", "yarn")  # æˆ–è€… "local[*]" ç”¨äºæœ¬åœ°æµ‹è¯•
        conf.set("spark.submit.deployMode", "client")
        conf.set("spark.executor.memory", "2g")
        conf.set("spark.driver.memory", "2g")
        conf.set("spark.executor.cores", "2")
        conf.set("spark.task.maxFailures", "4")
        
        # è®¾ç½®Pythonç¯å¢ƒ
        conf.set("spark.yarn.appMasterEnv.PYTHONPATH", ":".join(sys.path))
        conf.set("spark.executorEnv.PYTHONPATH", ":".join(sys.path))
        
        spark = SparkSession.builder.config(conf=conf).getOrCreate()
        logger.info(f"PySpark session created. App ID: {spark.sparkContext.applicationId}")
        return spark
    except Exception as e:
        logger.error(f"Failed to create PySpark session: {e}")
        return None

def get_tenant_access_token(app_id, app_secret, logger):
    """è·å–é£ä¹¦åº”ç”¨çš„tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            token = result["tenant_access_token"]
            logger.info("æˆåŠŸè·å–tenant_access_token")
            return token
        else:
            logger.error(f"è·å–tenant_access_tokenå¤±è´¥: {result.get('msg')}")
            return None
    except Exception as e:
        logger.error(f"è·å–tenant_access_tokenæ—¶å‡ºé”™: {e}")
        return None

def get_ai_news_from_source(url, source_name="æœºå™¨ä¹‹å¿ƒ", logger=None):
    """ä»æŒ‡å®šURLè·å–AIæ–°é—»"""
    try:
        if logger:
            logger.info(f"æ­£åœ¨ä» {source_name} è·å–æ–°é—»...")
        
        # æ ¹æ®ä¸åŒçš„æ•°æ®æºä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
        if source_name == "æœºå™¨ä¹‹å¿ƒ":
            return get_jiqizhixin_news(url, source_name, logger)
        elif source_name == "36æ°ª":
            return get_36kr_news(url, source_name, logger)
        elif source_name == "InfoQ":
            return get_infoq_news(url, source_name, logger)
        elif source_name == "AMiner":
            return get_aminer_news(url, source_name, logger)
        elif source_name == "é›·é”‹ç½‘":
            return get_leiphone_news(url, source_name, logger)
        elif source_name == "VentureBeat":
            return get_venturebeat_news(url, source_name, logger)
        elif source_name == "TechCrunch":
            return get_techcrunch_news(url, source_name, logger)
        elif source_name.endswith("RSS") or "rss" in source_name.lower():
            return get_rss_news(url, source_name, logger)
        elif source_name.endswith("API") or "api" in source_name.lower():
            return get_api_news(url, source_name, logger)
        else:
            return get_generic_news(url, source_name, logger)
    except Exception as e:
        if logger:
            logger.error(f"ä» {source_name} è·å–æ–°é—»æ—¶å‡ºé”™ {url}: {e}")
        return []

def get_jiqizhixin_news(url, source_name, logger=None):
    """è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»"""
    try:
        if logger: logger.debug(f"åˆå§‹åŒ–Chromeæµè§ˆå™¨é…ç½®...")
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-dev-tools-bridge")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--disable-javascript-har-promises")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--no-first-run")
        chrome_options.add_argument("--disable-default-apps")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-features=TranslateUI")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        # è®¾ç½®äºŒè¿›åˆ¶è·¯å¾„å’Œé©±åŠ¨
        chrome_options.binary_location = "/usr/bin/google-chrome-stable"
        service = Service(executable_path="/usr/bin/chromedriver")
        
        if logger: logger.debug(f"å¯åŠ¨Chromeæµè§ˆå™¨...")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        if logger: logger.debug(f"æ­£åœ¨è®¿é—®é¡µé¢: {url}")
        driver.get(url)
        
        if logger: logger.debug(f"ç­‰å¾…é¡µé¢åŠ è½½...")
        wait = WebDriverWait(driver, 30)
        try:
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "home__left-body")))
        except:
            if logger: logger.warning(f"æœªæ‰¾åˆ°home__left-bodyå…ƒç´ ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨")
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
        
        if logger: logger.debug(f"è·å–é¡µé¢æºç ...")
        page_source = driver.page_source
        if logger: logger.debug(f"é¡µé¢æºç é•¿åº¦: {len(page_source)} å­—ç¬¦")
        
        if logger: logger.debug(f"å…³é—­æµè§ˆå™¨...")
        driver.quit()
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_="body-title")
        
        if len(news_items) < 5:
            more_items = soup.find_all("a", class_=["article-item", "news-item", "post-title", "title"])
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            potential_titles = soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"title|heading|headline"))
            more_items = soup.find_all("a", href=re.compile(r"articles|news|post|reference"))
            news_items.extend(more_items)
        
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            ai_links = []
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and ("ai" in text.lower() or "æœºå™¨" in text or "æ™ºèƒ½" in text or "AI" in text):
                    ai_links.append(link)
            news_items.extend(ai_links)
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.jiqizhixin.com" + link
                    else:
                        link = "https://www.jiqizhixin.com/" + link
            else:
                onclick = item.get("onclick", "")
                if onclick:
                    match = re.search(r"'(https?://[^']+)'", onclick)
                    if match:
                        link = match.group(1)
                    else:
                        match = re.search(r"'(/articles/[^']+)'", onclick)
                        if match:
                            link = "https://www.jiqizhixin.com" + match.group(1)
                if not link:
                    data_href = item.get("data-href", "")
                    if data_href:
                        if not data_href.startswith("http"):
                            if data_href.startswith("/"):
                                link = "https://www.jiqizhixin.com" + data_href
                            else:
                                link = "https://www.jiqizhixin.com/" + data_href
                        else:
                            link = data_href
            
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            parent = item.find_parent()
            if parent:
                date_elements = parent.find_all(string=re.compile(r"\d{1,2}æœˆ\d{1,2}æ—¥"))
                if date_elements:
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", str(date_elements[0]))
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–æœºå™¨ä¹‹å¿ƒæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_36kr_news(url, source_name, logger=None):
    """è·å–36æ°ªAIæ–°é—»"""
    try:
        # å…ˆå°è¯•ä½¿ç”¨requestsè·å–
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„ç±»åå’Œé€‰æ‹©å™¨
        news_items = []
        
        # æ–¹æ³•1ï¼šå¯»æ‰¾æ–‡ç« é“¾æ¥
        news_items.extend(soup.find_all("a", class_=["item-title", "article-title", "title", "post-title"]))
        
        # æ–¹æ³•2ï¼šå¯»æ‰¾åŒ…å«AIå…³é”®è¯çš„é“¾æ¥
        if len(news_items) < 5:
            all_links = soup.find_all("a", href=True)
            for link in all_links:
                text = link.get_text(strip=True)
                if len(text) > 10 and any(keyword in text.lower() for keyword in AI_KEYWORDS):
                    news_items.append(link)
        
        # æ–¹æ³•3ï¼šå¯»æ‰¾ç‰¹å®šçš„æ–‡ç« å®¹å™¨
        if len(news_items) < 5:
            news_items.extend(soup.find_all("div", class_=re.compile(r"item|article|post")))
        
        for item in news_items:
            # å¦‚æœæ˜¯divå®¹å™¨ï¼Œéœ€è¦ä»ä¸­æå–é“¾æ¥
            if item.name == "div":
                link_element = item.find("a")
                if not link_element:
                    continue
                title = link_element.get_text(strip=True)
                link = link_element.get("href", "")
            else:
                title = item.get_text(strip=True)
                link = item.get("href", "")
            
            if len(title) < 5:
                continue
                
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://36kr.com" + link
                    else:
                        link = "https://36kr.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ—¥æœŸ
            parent = item.find_parent()
            if parent:
                time_element = parent.find("time") or parent.find("span", class_=re.compile(r"time|date"))
                if time_element:
                    date_text = time_element.get_text(strip=True)
                    date_match = re.search(r"\d{1,2}æœˆ\d{1,2}æ—¥", date_text)
                    if date_match:
                        date = date_match.group(0)
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–36æ°ªæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_infoq_news(url, source_name, logger=None):
    """è·å–InfoQ AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["news-title", "article-title", "title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.infoq.cn" + link
                    else:
                        link = "https://www.infoq.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–InfoQæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_aminer_news(url, source_name, logger=None):
    """è·å–AMiner AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "paper-title", "article-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.aminer.cn" + link
                    else:
                        link = "https://www.aminer.cn/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–AMineræ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_leiphone_news(url, source_name, logger=None):
    """è·å–é›·é”‹ç½‘AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link:
                if not link.startswith("http"):
                    if link.startswith("/"):
                        link = "https://www.leiphone.com" + link
                    else:
                        link = "https://www.leiphone.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–é›·é”‹ç½‘æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_venturebeat_news(url, source_name, logger=None):
    """è·å–VentureBeat AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://venturebeat.com" + link
                else:
                    link = "https://venturebeat.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–VentureBeatæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_techcrunch_news(url, source_name, logger=None):
    """è·å–TechCrunch AIæ–°é—»"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if link and not link.startswith("http"):
                if link.startswith("/"):
                    link = "https://techcrunch.com" + link
                else:
                    link = "https://techcrunch.com/" + link
            
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            if not any(keyword in title.lower() for keyword in AI_KEYWORDS):
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–TechCrunchæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_generic_news(url, source_name, logger=None):
    """é€šç”¨æ–°é—»è·å–æ–¹æ³•"""
    try:
        if logger: logger.debug(f"åˆå§‹åŒ–Chromeæµè§ˆå™¨é…ç½®(é€šç”¨æ–¹æ³•)...")
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-dev-tools-bridge")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--disable-javascript-har-promises")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--no-first-run")
        chrome_options.add_argument("--disable-default-apps")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-features=TranslateUI")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        # è®¾ç½®äºŒè¿›åˆ¶è·¯å¾„å’Œé©±åŠ¨
        chrome_options.binary_location = "/usr/bin/google-chrome-stable"
        service = Service(executable_path="/usr/bin/chromedriver")
        
        if logger: logger.debug(f"å¯åŠ¨Chromeæµè§ˆå™¨(é€šç”¨æ–¹æ³•)...")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        if logger: logger.debug(f"æ­£åœ¨è®¿é—®é¡µé¢(é€šç”¨æ–¹æ³•): {url}")
        driver.get(url)
        
        if logger: logger.debug(f"ç­‰å¾…é¡µé¢åŠ è½½(é€šç”¨æ–¹æ³•)...")
        wait = WebDriverWait(driver, 30)
        try:
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        except:
            if logger: logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å¤„ç†")
        
        if logger: logger.debug(f"è·å–é¡µé¢æºç (é€šç”¨æ–¹æ³•)...")
        page_source = driver.page_source
        if logger: logger.debug(f"é¡µé¢æºç é•¿åº¦(é€šç”¨æ–¹æ³•): {len(page_source)} å­—ç¬¦")
        
        if logger: logger.debug(f"å…³é—­æµè§ˆå™¨(é€šç”¨æ–¹æ³•)...")
        driver.quit()
        
        soup = BeautifulSoup(page_source, "html.parser")
        articles = []
        
        news_items = soup.find_all("a", class_=["title", "article-title", "entry-title", "post-title"])
        
        for item in news_items:
            title = item.get_text(strip=True)
            if len(title) < 5:
                continue
                
            link = item.get("href", "")
            if not link:
                continue
            
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            
            articles.append({"title": title, "link": link, "date": date, "source": source_name})
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–é€šç”¨æ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_rss_news(url, source_name, logger=None):
    """è·å–RSS/ATOMæ–°é—»"""
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries:
            title = entry.get('title', '')
            if not title or len(title.strip()) < 5:
                continue
                
            link = entry.get('link', '')
            if not link:
                continue
            
            # ç­›é€‰AIç›¸å…³æ–°é—»
            title_lower = title.lower()
            ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                          "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                          "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
            
            if not any(keyword in title_lower for keyword in ai_keywords):
                continue
            
            # è·å–æ—¥æœŸ
            date = datetime.now().strftime("%mæœˆ%dæ—¥")
            if 'published' in entry:
                try:
                    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z')
                    date = pub_date.strftime("%mæœˆ%dæ—¥")
                except:
                    pass
            
            articles.append({
                "title": title.strip(),
                "link": link,
                "date": date,
                "source": source_name
            })
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–RSSæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_api_news(url, source_name, logger=None):
    """è·å–APIæ–°é—»"""
    try:
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        articles = []
        
        # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
        if isinstance(data, dict):
            # å°è¯•ä¸åŒçš„APIå“åº”æ ¼å¼
            articles_data = []
            
            # æ ¼å¼1: {articles: [...]}
            if 'articles' in data:
                articles_data = data['articles']
            # æ ¼å¼2: {data: [...]}
            elif 'data' in data and isinstance(data['data'], list):
                articles_data = data['data']
            # æ ¼å¼3: {items: [...]}
            elif 'items' in data:
                articles_data = data['items']
            # æ ¼å¼4: {results: [...]}
            elif 'results' in data:
                articles_data = data['results']
            # æ ¼å¼5: {response: {docs: [...]}}
            elif 'response' in data and isinstance(data['response'], dict) and 'docs' in data['response']:
                articles_data = data['response']['docs']
            
            for item in articles_data:
                if isinstance(item, dict):
                    title = item.get('title', item.get('headline', item.get('name', '')))
                    link = item.get('url', item.get('link', item.get('href', '')))
                    
                    if not title or len(title.strip()) < 5:
                        continue
                    
                    if not link:
                        continue
                    
                    # ç­›é€‰AIç›¸å…³æ–°é—»
                    title_lower = title.lower()
                    ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural", 
                                  "algorithm", "model", "chatgpt", "openai", "gpt", "transformer", "llm",
                                  "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ™ºèƒ½", "ç®—æ³•", "å¤§æ¨¡å‹"]
                    
                    if not any(keyword in title_lower for keyword in ai_keywords):
                        continue
                    
                    # è·å–æ—¥æœŸ
                    date = datetime.now().strftime("%mæœˆ%dæ—¥")
                    pub_date = item.get('publishedAt', item.get('published_date', item.get('date', '')))
                    if pub_date:
                        try:
                            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
                            for date_format in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d %H:%M:%S']:
                                try:
                                    pub_date_obj = datetime.strptime(pub_date, date_format)
                                    date = pub_date_obj.strftime("%mæœˆ%dæ—¥")
                                    break
                                except:
                                    continue
                        except:
                            pass
                    
                    articles.append({
                        "title": title.strip(),
                        "link": link,
                        "date": date,
                        "source": source_name
                    })
        
        return articles
    except Exception as e:
        if logger: logger.error(f"è·å–APIæ–°é—»æ—¶å‡ºé”™: {e}")
        return []

def get_ai_news(logger=None):
    """ä»å¤šä¸ªæ•°æ®æºè·å–AIæ–°é—»ï¼ŒæŒ‰ç…§ä¼˜å…ˆçº§æ’åº"""
    all_articles = []
    
    # ä»é…ç½®æ–‡ä»¶è·å–å¯ç”¨çš„æ•°æ®æºï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
    enabled_sources = [source for source in NEWS_SOURCES if source.get("enabled", True)]
    enabled_sources.sort(key=lambda x: x.get("priority", 3), reverse=True)
    
    # ä»æ¯ä¸ªæ•°æ®æºè·å–æ–°é—»
    for source in enabled_sources:
        logger.info(f"æ­£åœ¨ä» {source['name']} (ä¼˜å…ˆçº§: {source.get('priority', 3)}) è·å–æ–°é—»...")
        articles = get_ai_news_from_source(source["url"], source["name"], logger)
        
        # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ ä¼˜å…ˆçº§ä¿¡æ¯
        priority = source.get("priority", 3)
        for article in articles:
            article["priority"] = priority
        
        # é™åˆ¶æ¯ä¸ªæºçš„æ–‡ç« æ•°é‡
        articles = articles[:MAX_ARTICLES_PER_SOURCE]
        all_articles.extend(articles)
    
    # å»é‡ - ä¼˜å…ˆåŸºäºæ ‡é¢˜å»é‡ï¼Œé¿å…é‡å¤å†…å®¹
    seen_titles = set()
    unique_articles = []
    for article in all_articles:
        # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        clean_title = article["title"].strip()
        # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå»é‡çš„ä¸»è¦ä¾æ®
        if clean_title not in seen_titles and len(clean_title) > 10:
            seen_titles.add(clean_title)
            unique_articles.append(article)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºæ–‡ç« 
    unique_articles.sort(key=lambda x: x.get("priority", 3), reverse=True)
    
    # æ ¹æ®ä¼˜å…ˆçº§é™åˆ¶æ–‡ç« æ•°é‡
    priority_articles = []
    priority_counts = {5: 0, 4: 0, 3: 0, 2: 0, 1: 0}
    
    for article in unique_articles:
        priority = article.get("priority", 3)
        if priority_counts[priority] < MAX_ARTICLES_PER_PRIORITY.get(priority, 4):
            priority_articles.append(article)
            priority_counts[priority] += 1
        
        # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œæå‰é€€å‡º
        if len(priority_articles) >= MAX_TOTAL_ARTICLES:
            break
    
    return priority_articles

def summarize_news(news_list, logger=None, provider_name=None):
    """ä½¿ç”¨AIå¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‘˜è¦"""
    try:
        # è·å–æ¨¡å‹å®¢æˆ·ç«¯
        client = get_model_client(provider_name, logger)
        if not client:
            if logger:
                logger.error("æ— æ³•è·å–æ¨¡å‹å®¢æˆ·ç«¯")
            else:
                print("æ— æ³•è·å–æ¨¡å‹å®¢æˆ·ç«¯")
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼šæ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥"
        
        # è·å–ä¾›åº”å•†é…ç½®
        if provider_name is None:
            provider_name = CURRENT_PROVIDER
        provider_config = MODEL_PROVIDERS[provider_name]
        
        # æ„å»ºåŒ…å«æ¥æºä¿¡æ¯çš„æ–°é—»æ–‡æœ¬
        news_items = []
        for news in news_list:
            if "source" in news:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ¥æº: {news["source"]}, æ—¥æœŸ: {news["date"]})')
            else:
                news_items.append(f'æ ‡é¢˜: {news["title"]} (æ—¥æœŸ: {news["date"]})')
        news_text = "\n".join(news_items)
        
        response = client.chat.completions.create(
            model=provider_config["model"],
            messages=[
                {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹AIæ–°é—»ï¼Œæå–å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿:\n{news_text}"}
            ],
            max_tokens=provider_config["max_tokens"]
        )
        if logger:
            logger.debug(f"APIå“åº”: {response}")
        return response.choices[0].message.content.strip()
    except Exception as e:
        if logger:
            logger.error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        else:
            print(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

def send_to_feishu(webhook_url, summary, news_list, image_key=None, logger=None):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤èŠï¼Œæ”¯æŒå¡ç‰‡æ¶ˆæ¯æ ¼å¼"""
    try:
        headers = {
            "Content-Type": "application/json"
        }
        
        # å¤„ç†æ‘˜è¦å†…å®¹ï¼Œä¿ç•™å¿…è¦çš„æ ¼å¼æ ‡è®°
        clean_summary = summary.replace('### **å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**', '**å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“**')  # ç‰¹æ®Šå¤„ç†ä¸»æ ‡é¢˜
        clean_summary = clean_summary.replace('### **æ ¸å¿ƒè¶‹åŠ¿æç‚¼**', '**æ ¸å¿ƒè¶‹åŠ¿æç‚¼**')  # ç‰¹æ®Šå¤„ç†æ ¸å¿ƒè¶‹åŠ¿æ ‡é¢˜
        clean_summary = clean_summary.replace('### ', '')  # ç§»é™¤å…¶ä»–ä¸»æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#### ', '')  # ç§»é™¤å­æ ‡é¢˜æ ‡è®°
        clean_summary = clean_summary.replace('#', '')  # ç§»é™¤äº•å·æ ‡è®°
        clean_summary = clean_summary.replace('`', '')  # ç§»é™¤åå¼•å·æ ‡è®°
        clean_summary = clean_summary.replace('_', '')  # ç§»é™¤ä¸‹åˆ’çº¿æ ‡è®°
        clean_summary = clean_summary.replace('~', '')  # ç§»é™¤æ³¢æµªçº¿æ ‡è®°
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯å†…å®¹
        card_elements = []
        
        # ç§»é™¤äº†å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
        
        # ç§»é™¤äº†é‡å¤çš„æ—¥æœŸæ˜¾ç¤ºï¼Œé¿å…ä¸å¡ç‰‡å¤´éƒ¨æ ‡é¢˜é‡å¤
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ‘˜è¦æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“Š ä»Šæ—¥AIçƒ­ç‚¹æ‘˜è¦:**"
            }
        })
        
        # æ·»åŠ æ‘˜è¦å†…å®¹ï¼ˆä¼˜åŒ–æ ‡é¢˜å’Œæ­£æ–‡çš„æ ¼å¼å¤„ç†ï¼‰
        # æŒ‰æ®µè½åˆ†å‰²
        summary_paragraphs = clean_summary.split('\n\n')
        for paragraph in summary_paragraphs:
            if paragraph.strip():
                # å¤„ç†æ®µè½ä¸­çš„æ¢è¡Œ
                lines = paragraph.split('\n')
                formatted_lines = []
                for line in lines:
                    if line.strip():
                        # ä¿ç•™åŸæœ‰çš„ç²—ä½“æ ‡è®°
                        formatted_lines.append(line.strip())
                
                formatted_paragraph = '\n'.join(formatted_lines)
                
                # è¯†åˆ«å¹¶å¤„ç†æ ‡é¢˜
                stripped_paragraph = formatted_paragraph.strip()
                if stripped_paragraph.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) and len(stripped_paragraph) < 100:
                    # æ•°å­—å¼€å¤´çš„çŸ­æ®µè½ï¼Œè®¤ä¸ºæ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif stripped_paragraph.startswith("å…³é”®ä¿¡æ¯ä¸è¶‹åŠ¿æ€»ç»“") or stripped_paragraph.startswith("æ ¸å¿ƒè¶‹åŠ¿æ€»ç»“"):
                    # ç‰¹æ®Šæ ‡é¢˜å¤„ç†
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                elif len(stripped_paragraph) < 50 and ':' in stripped_paragraph and stripped_paragraph.count(':') <= 1:
                    # çŸ­æ®µè½ä¸”åŒ…å«å•ä¸ªå†’å·ï¼Œå¯èƒ½æ˜¯å°èŠ‚æ ‡é¢˜
                    if not stripped_paragraph.startswith('**'):
                        formatted_paragraph = f"**{stripped_paragraph}**"
                
                card_elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": formatted_paragraph
                    }
                })
        
        # æ·»åŠ åˆ†å‰²çº¿
        card_elements.append({
            "tag": "hr"
        })
        
        # æ·»åŠ æ–°é—»åˆ—è¡¨æ ‡é¢˜
        card_elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° è¯¦ç»†æ–°é—»åˆ—è¡¨:**"
            }
        })
        
        # æ·»åŠ æ–°é—»æ¡ç›®
        for i, news in enumerate(news_list[:10], 1):  # é™åˆ¶æœ€å¤šæ˜¾ç¤º10æ¡æ–°é—»
            # æ„å»ºæ–°é—»æ¡ç›®å†…å®¹ï¼ŒåŒ…å«æ¥æºä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "source" in news:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ¥æº: {news['source']} æ—¥æœŸ: {news['date']}"
            else:
                news_content = f"{i}. [{news['title']}]({news['link']}) æ—¥æœŸ: {news['date']}"
                
            card_elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": news_content
                }
            })
        
        # æ„å»ºå¡ç‰‡æ¶ˆæ¯
        data = {
            "msg_type": "interactive",
            "card": {
                "config": {
                    "wide_screen_mode": True,
                    "enable_forward": True
                },
                "header": {
                    "template": "blue",
                    "title": {
                        "content": "ğŸ¤– AIæ—¥æŠ¥ - {}".format(datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")),
                        "tag": "plain_text"
                    }
                },
                "elements": card_elements
            }
        }
            
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data, ensure_ascii=False).encode('utf-8'))
        response.raise_for_status()
        if logger:
            logger.info("æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
        else:
            print("æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
        return True
    except Exception as e:
        if logger:
            logger.error(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        else:
            print(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return False

def upload_image_to_feishu(image_path, access_token, logger=None):
    """ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦å¹¶è·å–image_key"""
    try:
        upload_url = "https://open.feishu.cn/open-apis/im/v1/images"
        
        headers = {
            "Authorization": f"Bearer {access_token}",
        }
        
        files = {
            "image_type": (None, "message"),
            "image": (os.path.basename(image_path), open(image_path, "rb"), "image/jpeg") 
        }
        
        response = requests.post(upload_url, headers=headers, files=files)
        response.raise_for_status()
        
        result = response.json()
        if result.get("code") == 0:
            image_key = result["data"]["image_key"]
            if logger:
                logger.info(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œimage_key: {image_key}")
            else:
                print(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œimage_key: {image_key}")
            return image_key
        else:
            if logger:
                logger.error(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {result.get('msg')}")
            else:
                print(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {result.get('msg')}")
            return None
    except Exception as e:
        if logger:
            logger.error(f"ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        else:
            print(f"ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
        return None

# å¤šwebhooké…ç½®
WEBHOOK_CONFIGS = [
    {
        "name": "ä¸»ç¾¤èŠ",
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/02880094-6e28-4dea-a815-ff02fea49072",
        "enabled": True,
        "send_image": True
    },
    {
        "name": "æµ‹è¯•ç¾¤èŠ", 
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/your_test_webhook_url_here",
        "enabled": False,
        "send_image": False
    },
    {
        "name": "å¤‡ä»½ç¾¤èŠ",
        "url": "https://open.feishu.cn/open-apis/bot/v2/hook/your_backup_webhook_url_here", 
        "enabled": False,
        "send_image": False
    }
]

def print_model_provider_configs(logger=None):
    """æ‰“å°å½“å‰æ¨¡å‹ä¾›åº”å•†é…ç½®çŠ¶æ€"""
    if logger:
        logger.info("\n" + "="*50)
        logger.info("å½“å‰æ¨¡å‹ä¾›åº”å•†é…ç½®çŠ¶æ€:")
        logger.info("="*50)
        
        for provider_name, config in MODEL_PROVIDERS.items():
            status = "âœ… å¯ç”¨" if config.get("enabled", True) else "âŒ ç¦ç”¨"
            current = "ğŸ‘‰ å½“å‰ä½¿ç”¨" if provider_name == CURRENT_PROVIDER else ""
            logger.info(f"ğŸ¤– {config['name']} ({provider_name}): {status} {current}")
            logger.info(f"   æ¨¡å‹: {config['model']}")
            logger.info(f"   æœ€å¤§Token: {config['max_tokens']}")
            logger.info("-" * 30)
        
        logger.info(f"ğŸ“Š å½“å‰ä½¿ç”¨: {MODEL_PROVIDERS[CURRENT_PROVIDER]['name']}")
        logger.info("="*50 + "\n")
    else:
        print("\n" + "="*50)
        print("å½“å‰æ¨¡å‹ä¾›åº”å•†é…ç½®çŠ¶æ€:")
        print("="*50)
        
        for provider_name, config in MODEL_PROVIDERS.items():
            status = "âœ… å¯ç”¨" if config.get("enabled", True) else "âŒ ç¦ç”¨"
            current = "ğŸ‘‰ å½“å‰ä½¿ç”¨" if provider_name == CURRENT_PROVIDER else ""
            print(f"ğŸ¤– {config['name']} ({provider_name}): {status} {current}")
            print(f"   æ¨¡å‹: {config['model']}")
            print(f"   æœ€å¤§Token: {config['max_tokens']}")
            print("-" * 30)
        
        print(f"ğŸ“Š å½“å‰ä½¿ç”¨: {MODEL_PROVIDERS[CURRENT_PROVIDER]['name']}")
        print("="*50 + "\n")

def print_webhook_configs(logger=None):
    """æ‰“å°å½“å‰webhooké…ç½®çŠ¶æ€"""
    if logger:
        logger.info("\n" + "="*50)
        logger.info("å½“å‰Webhooké…ç½®çŠ¶æ€:")
        logger.info("="*50)
        
        enabled_count = 0
        for config in WEBHOOK_CONFIGS:
            status = "âœ… å¯ç”¨" if config["enabled"] else "âŒ ç¦ç”¨"
            image_status = "âœ… å‘é€" if config["send_image"] else "âŒ ä¸å‘é€"
            logger.info(f"ğŸ“Œ {config['name']}: {status}")
            logger.info(f"   URL: {config['url']}")
            logger.info(f"   å›¾ç‰‡: {image_status}")
            logger.info("-" * 30)
            if config["enabled"]:
                enabled_count += 1
        
        logger.info(f"ğŸ“Š æ€»è®¡: {enabled_count}/{len(WEBHOOK_CONFIGS)} ä¸ªwebhookå·²å¯ç”¨")
        logger.info("="*50 + "\n")
    else:
        # å¦‚æœæ²¡æœ‰loggerï¼Œä½¿ç”¨printä½œä¸ºåå¤‡
        print("\n" + "="*50)
        print("å½“å‰Webhooké…ç½®çŠ¶æ€:")
        print("="*50)
        
        enabled_count = 0
        for config in WEBHOOK_CONFIGS:
            status = "âœ… å¯ç”¨" if config["enabled"] else "âŒ ç¦ç”¨"
            image_status = "âœ… å‘é€" if config["send_image"] else "âŒ ä¸å‘é€"
            print(f"ğŸ“Œ {config['name']}: {status}")
            print(f"   URL: {config['url']}")
            print(f"   å›¾ç‰‡: {image_status}")
            print("-" * 30)
            if config["enabled"]:
                enabled_count += 1
        
        print(f"ğŸ“Š æ€»è®¡: {enabled_count}/{len(WEBHOOK_CONFIGS)} ä¸ªwebhookå·²å¯ç”¨")
        print("="*50 + "\n")

def send_to_multiple_webhooks(summary, news_list, image_key=None, logger=None):
    """å¹¶å‘å‘é€æ¶ˆæ¯åˆ°å¤šä¸ªwebhook"""
    import concurrent.futures

    print_webhook_configs(logger)
    
    # è·å–å¯ç”¨çš„webhook
    enabled_webhooks = [config for config in WEBHOOK_CONFIGS if config["enabled"]]
    
    if not enabled_webhooks:
        if logger:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„webhooké…ç½®")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„webhooké…ç½®")
        return False
    
    if logger:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘å‘é€åˆ° {len(enabled_webhooks)} ä¸ªwebhook...")
    else:
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å‘é€åˆ° {len(enabled_webhooks)} ä¸ªwebhook...")
    
    success_count = 0
    failure_count = 0
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å‘é€
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(enabled_webhooks), 5)) as executor:
        # åˆ›å»ºå‘é€ä»»åŠ¡
        future_to_config = {}
        for config in enabled_webhooks:
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å‘é€å›¾ç‰‡
            webhook_image_key = image_key if config["send_image"] else None
            future = executor.submit(send_to_feishu, config["url"], summary, news_list, webhook_image_key, config["name"])
            future_to_config[future] = config
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in concurrent.futures.as_completed(future_to_config):
            config = future_to_config[future]
            try:
                result = future.result()
                
                # æ ‡å‡†åŒ–å¤„ç†è¿”å›å€¼
                if isinstance(result, bool):
                    success = result
                    error_msg = None
                elif isinstance(result, str):
                    success = False
                    error_msg = result
                else:
                    success = False
                    error_msg = f"è¿”å›å€¼ç±»å‹é”™è¯¯: {type(result)}"
                
                results.append({
                    "name": config["name"],
                    "success": success,
                    "error": error_msg
                })
                
                if success:
                    success_count += 1
                    log_msg = f"âœ… {config['name']}: å‘é€æˆåŠŸ"
                else:
                    failure_count += 1
                    log_msg = f"âŒ {config['name']}: å‘é€å¤±è´¥"
                    if error_msg:
                        log_msg += f" - {error_msg}"
                
                if logger:
                    logger.info(log_msg)
                else:
                    print(log_msg)
                    
            except Exception as e:
                failure_count += 1
                error_msg = str(e)
                results.append({
                    "name": config["name"],
                    "success": False,
                    "error": error_msg
                })
                log_msg = f"âŒ {config['name']}: å‘é€å¼‚å¸¸ - {error_msg}"
                if logger:
                    logger.error(log_msg)
                else:
                    print(log_msg)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    if logger:
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š å‘é€ç»“æœç»Ÿè®¡:")
        logger.info("="*50)
        logger.info(f"âœ… æˆåŠŸ: {success_count}")
        logger.info(f"âŒ å¤±è´¥: {failure_count}")
        logger.info(f"ğŸ“ˆ æ€»è®¡: {success_count + failure_count}")
    else:
        print("\n" + "="*50)
        print("ğŸ“Š å‘é€ç»“æœç»Ÿè®¡:")
        print("="*50)
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ å¤±è´¥: {failure_count}")
        print(f"ğŸ“ˆ æ€»è®¡: {success_count + failure_count}")
    
    if failure_count > 0:
        if logger:
            logger.warning("\nâŒ å¤±è´¥è¯¦æƒ…:")
            for result in results:
                if not result["success"]:
                    error_info = result.get('error', 'æœªçŸ¥é”™è¯¯') if isinstance(result, dict) else 'ç»“æœæ ¼å¼é”™è¯¯'
                    logger.warning(f"   - {result.get('name', 'æœªçŸ¥webhook')}: {error_info}")
        else:
            print("\nâŒ å¤±è´¥è¯¦æƒ…:")
            for result in results:
                if not result["success"]:
                    error_info = result.get('error', 'æœªçŸ¥é”™è¯¯') if isinstance(result, dict) else 'ç»“æœæ ¼å¼é”™è¯¯'
                    print(f"   - {result.get('name', 'æœªçŸ¥webhook')}: {error_info}")
    
    if logger:
        logger.info("="*50 + "\n")
    else:
        print("="*50 + "\n")
    
    # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸï¼Œå°±è¿”å›True
    return success_count > 0

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    try:
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(LOG_CONFIG["file"])
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, LOG_CONFIG["level"]),
            format=LOG_CONFIG["format"],
            handlers=[
                logging.FileHandler(LOG_CONFIG["file"], encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )

        logger = logging.getLogger(__name__)
        logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return logger
    except Exception as e:
        print(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")  # è¿™é‡Œä¸èƒ½ä½¿ç”¨loggerï¼Œå› ä¸ºloggerè¿˜æœªåˆå§‹åŒ–
        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„logger
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)

def main():
    """ä¸»å‡½æ•° - æ”¯æŒPySparkç¯å¢ƒ"""
    start_time = datetime.now()
    logger = setup_logging()

    logger.info("=" * 60)
    logger.info(f"[START] å¼€å§‹æ‰§è¡ŒAIæ—¥æŠ¥ä»»åŠ¡ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)
    logger.info(f"æ—¥å¿—çº§åˆ«: {LOG_CONFIG['level']}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {LOG_CONFIG['file']}")
    
    # æ‰“å°æ¨¡å‹ä¾›åº”å•†é…ç½®
    print_model_provider_configs(logger)
    
    # åˆå§‹åŒ–PySparkç¯å¢ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
    spark = None
    if PYSPARK_AVAILABLE:
        logger.info("æ­£åœ¨åˆå§‹åŒ–PySparkç¯å¢ƒ...")
        spark = setup_pyspark_environment()
        if not spark:
            logger.warning("PySparkåˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§åˆ°ç‹¬ç«‹æ¨¡å¼è¿è¡Œ")
    else:
        logger.info("[INFO] è¿è¡Œåœ¨ç‹¬ç«‹æ¨¡å¼ï¼ˆéPySparkç¯å¢ƒï¼‰")
    
    try:
        # é£ä¹¦åº”ç”¨å‡­è¯
        feishu_app_id = "cli_a8ef4e27bd85900b"
        feishu_app_secret = "By4Y7Z2NpQvovyJ0Efp2CgyOF8dAC7bV"
        logger.info(f"[INFO] é£ä¹¦åº”ç”¨ID: {feishu_app_id}")
        
        # è·å–tenant_access_token
        logger.info("[INFO] æ­£åœ¨è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ...")
        access_token = get_tenant_access_token(feishu_app_id, feishu_app_secret, logger)
        if not access_token:
            logger.error("[ERROR] æ— æ³•è·å–access_tokenï¼Œé€€å‡ºä»»åŠ¡ã€‚")
            return
        logger.info("[SUCCESS] æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œ")

        # è·å–AIæ–°é—»ï¼ˆä»å¤šä¸ªæ•°æ®æºï¼‰
        logger.info("[INFO] å¼€å§‹è·å–AIæ–°é—»...")
        ai_news = get_ai_news(logger)
        
        if ai_news:
            logger.info(f"[SUCCESS] æˆåŠŸè·å– {len(ai_news)} æ¡æ–°é—»")
            
            # ç”Ÿæˆæ‘˜è¦
            logger.info("[INFO] æ­£åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦...")
            summary = summarize_news(ai_news, logger)
            
            logger.info("[SUCCESS] ç”Ÿæˆçš„æ—¥æŠ¥æ‘˜è¦:")
            logger.info("-" * 40)
            logger.info(summary)
            logger.info("-" * 40)
            
            # ä¸Šä¼ ä¸»é¢˜å›¾ç‰‡å¹¶å‘é€
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡å›¾ç‰‡ä¸Šä¼ 
            image_path = "/home/ubuntu/upload/search_images/QkPqdKuxZOlT.jpg" # é€‰æ‹©ä¸€å¼ AIç›¸å…³çš„å›¾ç‰‡
            image_key = None
            if os.path.exists(image_path):
                logger.info("[INFO] æ­£åœ¨ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦...")
                image_key = upload_image_to_feishu(image_path, access_token, logger)
            else:
                logger.warning(f"[WARNING] å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}ï¼Œå·²ç§»é™¤å›¾ç‰‡ä»¥ç¡®ä¿æ¶ˆæ¯å‘é€æˆåŠŸ")
                # å·²ç§»é™¤å›¾ç‰‡éƒ¨åˆ†ï¼Œé¿å…å¯èƒ½çš„å›¾ç‰‡é—®é¢˜å¯¼è‡´æ¶ˆæ¯å‘é€å¤±è´¥
            
            logger.info("[INFO] æ­£åœ¨å‘é€æ¶ˆæ¯åˆ°é£ä¹¦...")
            # ä½¿ç”¨å¤šwebhookå‘é€åŠŸèƒ½
            send_success = send_to_multiple_webhooks(summary, ai_news, image_key, logger)
            
            if send_success:
                logger.info("[SUCCESS] AIæ—¥æŠ¥å·²æˆåŠŸå‘é€åˆ°æ‰€æœ‰é…ç½®çš„webhook")
            else:
                logger.warning("[WARNING] éƒ¨åˆ†webhookå‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
        else:
            logger.error("[ERROR] æœªèƒ½è·å–AIæ–°é—»")
            
    except Exception as e:
        logger.error(f"[ERROR] ä¸»å‡½æ•°æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        logger.error(f"[ERROR] è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        
    finally:
        # æ¸…ç†PySparkèµ„æº
        if spark:
            logger.info("æ­£åœ¨æ¸…ç†PySparkèµ„æº...")
            try:
                spark.stop()
                logger.info("PySparkèµ„æºå·²æ¸…ç†")
            except Exception as e:
                logger.error(f"æ¸…ç†PySparkèµ„æºæ—¶å‡ºé”™: {e}")
        
        end_time = datetime.now()
        duration = end_time - start_time
        logger.info("=" * 60)
        logger.info(f"[END] AIæ—¥æŠ¥ä»»åŠ¡å®Œæˆ - {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {duration}")
        logger.info("=" * 60)

if __name__ == "__main__":
    main()

